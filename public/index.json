[{"content":"Si alguna vez te has sentido abrumado por términos como Data Lake, Metadata o Data Governance, entonces has llegado al lugar correcto.\nEntiendo que incluso un solo concepto puede tener diferentes interpretaciones, así que mi objetivo no es definir, sino explicar estos términos de manera sencilla con ejemplos o contexto.\nData Hechos, cifras o información en bruto que se recopilan y almacenan para su posterior análisis. Esto puede incluir hechos, números, mediciones, observaciones o cualquier otro detalle que se pueda utilizar para comprender un tema en particular.\nSi estás registrando la temperatura diaria en una ciudad durante el transcurso de un mes, podrías recopilar datos sobre la temperatura, la humedad, la velocidad del viento y las precipitaciones a lo largo del día. Los datos existen en diversas formas y se pueden encontrar en situaciones cotidianas.\nMetadata Es información que describe otros datos. Proporciona detalles sobre el contenido, la calidad, la condición, el origen y otras características de un elemento específico. En el contexto digital, los metadatos pueden incluir detalles como el autor, la fecha de creación, el tamaño del archivo y palabras clave.\nSi tienes fotos en tu smartphone, abre una imagen y podrás ver metadatos como la fecha en que fue tomada, el tamaño, la ubicación, etc. La foto en sí es el dato primario, mientras que los metadatos proporcionan información complementaria sobre la foto.\nDatabase Se trata de una colección organizada de información estructurada, que normalmente se almacena y se accede a ella electrónicamente desde un sistema informático. Permite el almacenamiento, la recuperación y la manipulación eficientes de los datos.\nEl ejemplo clásico es pensar en una biblioteca. Cada libro tiene su etiqueta con su título, autor, etc. En la biblioteca, se almacenan y organizan libros, pero en lugar de libros, piense en datos. Al igual que un sistema de archivos, una base de datos ayuda a mantener la información estructurada.\nStructured Data Se refiere a información que está organizada en un formato específico, lo que facilita su comprensión tanto para humanos como para computadoras. Consiste en elementos de datos organizados en filas y columnas, como una tabla.\nUna guía telefónica contiene datos estructurados porque organiza campos específicos como nombre, número de teléfono, dirección, etc.\nUnstructured Data Se refiere a información que no tiene un formato específico. Generalmente, los datos no estructurados carecen de una estructura predefinida, lo que hace que sean más difíciles de analizar y procesar en comparación con los datos estructurados.\nImagina una pila de cartas escritas a mano. Cada carta puede contener diferentes tipos de información, como historias personales, emociones u opiniones. Estas cartas no siguen un formato estandarizado y pueden variar en longitud, estilo de escritura o lenguaje.\nEn el mundo digital, los correos electrónicos, las publicaciones en las redes sociales, las imágenes, los videos, las grabaciones de audio y los documentos de texto de formato libre son ejemplos de datos no estructurados.\nSemi-structured Data Los datos semiestructurados se encuentran entre los datos estructurados y los no estructurados. Tienen cierta organización, pero carecen del formato predefinido estricto de los datos estructurados.\nPiensa en una bandeja de entrada de correo electrónico. Cada correo electrónico consta de elementos estructurados como remitente, destinatario, asunto y marca de tiempo. Sin embargo, el cuerpo del correo electrónico puede contener información no estructurada, como texto de formato libre, archivos adjuntos o diferentes estilos de formato. Esta combinación de estructura y flexibilidad representa los datos semiestructurados.\nData Testing Es el proceso de examinar y validar datos para garantizar su calidad, precisión y fiabilidad. Implica comprobar si los datos cumplen los estándares esperados y satisfacen los criterios deseados. Básicamente, la prueba de datos es como corregir los datos para detectar errores o inconsistencias, lo que garantiza que la información con la que se trabaja sea fiable y digna de confianza.\nImagina que trabajas como cajero en un supermercado y tu trabajo consiste en registrar los precios de los diferentes productos que se venden. Data testing en este contexto implicaría verificar que el precio que introduce para cada producto sea correcto, asegurándose de que no haya errores tipográficos ni equivocaciones. Puede comparar los precios introducidos con una lista de referencia o consultar con un compañero de trabajo para validar la precisión de los datos registrados.\nDuplicate Data Se refiere a tener múltiples copias idénticas o casi idénticas de la misma información dentro de un conjunto de datos o sistemas.\nConsidera el escenario en el que estás administrando tu lista de contactos. En esa lista, almacenas nombres, números de teléfono y correos electrónicos de tus amigos. Ahora, supongamos que agregas accidentalmente el mismo contacto dos veces con un typo, lo que da como resultado datos duplicados.\nOrphaned Data Se trata de datos que existen sin ningún contexto asociado o significativo. Básicamente, son datos que carecen de conexiones o relaciones adecuadas con otros elementos de datos.\nImagina una biblioteca en la que encuentras un libro al que le falta información sobre su autor, título, etc. Este libro se convierte en un libro huérfano porque no se puede categorizar ni utilizar adecuadamente, ya que carece de los detalles que lo harían valioso dentro del sistema de la biblioteca.\nDe manera similar, en el contexto de los datos, los datos huérfanos podrían ser una entrada sin ninguna información correspondiente, como un registro de cliente sin nombre ni datos de contacto. Estos datos se vuelven difíciles de analizar o utilizar de manera efectiva porque carecen del contexto necesario.\nIncomplete or Missing Data Se trata de datos que no están completamente disponibles o carecen de cierta información necesaria. Esto implica que existen lagunas en los datos que pueden dificultar su utilidad para el análisis o la toma de decisiones.\nImagina que estás viajando y utilizas una aplicación de navegación (como Google Maps) que proporciona tiempos de viaje estimados en función de datos históricos de tráfico. Sin embargo, si la aplicación no tiene información actualizada sobre el tráfico actual o los cierres de carreteras, no podrás predecir con precisión tu tiempo de viaje.\nEn este escenario, los datos incompletos o faltantes son la ausencia de información de tráfico en tiempo real. De manera similar, en el mundo de los datos, es posible que falten datos al analizar el comportamiento del cliente si no se capturan o registran ciertas variables.\nMislabeled Data Datos que han sido etiquetados o clasificados incorrectamente, lo que genera información inexacta o engañosa. Esto significa que los datos no representan con precisión su verdadera naturaleza o significado.\nEstás organizando una colección de fotos. Tienes una carpeta llamada \u0026ldquo;Vacaciones en Europa\u0026rdquo;, pero cuando la abres, encuentras imágenes de tus vacaciones en México. En el contexto de los datos, los datos mal etiquetados ocurren cuando se asignan etiquetas incorrectas.\nData Swamp Se trata de una situación en la que una gran cantidad de datos se desorganizan, desestructuran y dificultan su uso eficaz. Es un estado en el que los datos pierden valor y se estancan o se vuelven inutilizables debido a la falta de una gestión y organización adecuadas.\nImagina que estás en una habitación llena de diversos objetos, como ropa, zapatos, libros, etc. La habitación está desorganizada, lo que hace que sea muy difícil encontrar lo que necesitas. En este ejemplo, la habitación desordenada representa un pantano de datos.\nDe manera similar, en el ámbito digital, un pantano de datos puede surgir cuando hay una cantidad abrumadora de datos desorganizados almacenados en varios sistemas, bases de datos o archivos.\nData Temperature Es una clasificación que indica con qué frecuencia se accede y utiliza un conjunto de datos. Los datos \u0026ldquo;calientes\u0026rdquo; son de acceso frecuente, los \u0026ldquo;tibios\u0026rdquo; de acceso ocasional, y los \u0026ldquo;fríos\u0026rdquo; raramente se consultan. Esta clasificación ayuda a las organizaciones a optimizar el almacenamiento y los costos, manteniendo los datos más críticos fácilmente accesibles.\nImagina el refrigerador de tu casa. Guardas la leche y los alimentos que usas diariamente al frente (datos calientes), las sobras de ayer en el medio (datos tibios), y en el congelador al fondo tienes comida que raramente usas (datos fríos).\nEn el mundo empresarial, un banco mantendría las transacciones del día actual como datos calientes en sistemas rápidos, el historial del último año como datos tibios, y las transacciones de hace 10 años como datos fríos en almacenamiento más económico.\nData Lineage Es la capacidad de rastrear el origen y el movimiento de los datos a lo largo de su ciclo de vida. Esto le ayuda a comprender de dónde provienen los datos, cómo se transforman y a dónde van, lo que le permite rastrear y analizar el flujo de datos dentro de un sistema u organización.\nEn términos simples, el linaje de datos es como rastrear los pasos de sus datos, lo que le ayuda a comprender su recorrido de principio a fin y obtener información sobre cómo se usan y se transforman a lo largo del camino.\nImagina que pides un producto en línea. La plataforma de comercio electrónico procesa tu pedido, lo que implica varios pasos como la gestión de inventario, el procesamiento de pagos y el envío. El linaje de datos en este escenario implicaría rastrear el recorrido de los detalles de tu pedido desde el momento en que realizas el pedido hasta que llega a tu puerta.\nPor ejemplo, el linaje de datos podría mostrar que los detalles de tu pedido se originaron en la base de datos de la tienda en línea, luego se trasladaron al sistema de pago y, posteriormente, al departamento de logística para el envío.\nData Migration Es el proceso de transferir datos de un sistema, aplicación o ubicación de almacenamiento a otro. Implica mover datos desde su ubicación actual a un nuevo destino, asegurando su integridad, completitud y compatibilidad. En resumen, es como mover sus datos digitales de una ubicación a otra, de manera muy similar a mover elementos físicos de una casa a otra durante una mudanza.\nEs como mudarse de una casa antigua a una nueva. Como parte de la mudanza, debe transferir todas sus pertenencias, incluidos muebles, electrodomésticos y artículos personales de su antigua casa a la nueva. La migración de datos es similar a este proceso, pero en lugar de objetos físicos, implica mover datos digitales.\nDurante la migración, es importante considerar factores como la compatibilidad del formato de datos, la seguridad de los datos y la validación de los datos para garantizar una transferencia exitosa.\nData Model Es la representación estructurada de cómo se organizan, almacenan y relacionan los datos entre sí. Define la estructura lógica, las relaciones y las restricciones que deben seguir los datos. Funciona como un plano arquitectónico que guía el diseño y construcción de un sistema de datos.\nPiensa en el plano de una casa: muestra dónde van las habitaciones, cómo se conectan, y qué reglas existen (por ejemplo, la cocina debe estar cerca del comedor).\nEn datos, si diseñas un sistema para una escuela, tu modelo mostraría que cada estudiante tiene un nombre y matrícula, cada clase tiene un código y horario, y que existe una relación entre estudiantes y clases (un estudiante puede inscribirse en varias clases).\nEmpresas como Spotify usan modelos de datos para definir cómo se relacionan usuarios, canciones, playlists y artistas en su base de datos\nData Modeling Es el proceso de creación del modelo de datos. Implica analizar los requisitos, comprender las fuentes de datos y diseñar la estructura y las relaciones de los datos.\nEl \u0026lsquo;modelado de datos\u0026rsquo; puede considerarse como la actividad de traducir conceptos y entidades del mundo real en una representación formal de un modelo de datos.\nTienes una plataforma de comercio electrónico. Al crear un modelo de datos para dicha plataforma, el modelado de datos implicaría identificar y representar entidades, relaciones y atributos clave. Por ejemplo, un usuario representaría a una persona que se registra en la plataforma y puede tener atributos como ID, nombre, información de contacto y detalles del método de pago.\nSchema Es la estructura organizacional que define cómo se almacenan y relacionan los datos en una base de datos. Incluye las tablas, columnas, tipos de datos, restricciones y relaciones entre diferentes elementos. Funciona como el plano arquitectónico que establece las reglas de organización de la información.\nPiensa en un formulario de inscripción escolar: tiene campos específicos como nombre (texto), fecha de nacimiento (fecha), grado (número), y reglas como \u0026ldquo;el nombre es obligatorio\u0026rdquo; o \u0026ldquo;la fecha no puede ser futura\u0026rdquo;.\nUn schema hace exactamente eso con los datos. En una aplicación como Instagram, el schema define que cada usuario tiene un nombre de usuario único (texto), una fecha de registro (fecha), seguidores (número), y que cada foto tiene un usuario asociado (relación), garantizando que todos los datos sigan estas reglas y estructura consistente.\nSource System Es cualquier sistema, aplicación o base de datos que genera u origina datos que posteriormente serán consumidos por pipelines de ingeniería de datos. Puede ser una base de datos transaccional (OLTP), APIs, archivos, logs, sensores IoT, o aplicaciones SaaS. Comprender los source systems es crítico porque sus características (velocidad de generación, formato, schema) dictan las estrategias de ingesta y transformación downstream.\nImagina las fuentes de agua de una ciudad: manantiales, ríos, pozos, cada uno con diferentes características (calidad, volumen, constancia). Los sistemas fuente son igual de diversos.\nUna empresa de retail tiene múltiples source systems: su aplicación web genera eventos de clicks en tiempo real (streaming), su ERP guarda transacciones de ventas (base de datos transaccional), proveedores envían archivos CSV de inventario diariamente (batch files), y sensores en tiendas físicas registran tráfico de clientes (IoT). Cada uno requiere estrategias diferentes de ingesta: APIs para la web, CDC para el ERP, procesamiento batch para CSVs, y streaming para sensores, pero todos alimentan el ecosistema de datos de la empresa.\nSlowly Changing Dimensions (SCD) Son dimensiones en un data warehouse cuyo contenido cambia lentamente y de forma impredecible a lo largo del tiempo, requiriendo estrategias específicas para rastrear cambios históricos. Los tipos más comunes son: Tipo 1 (sobrescribir), Tipo 2 (agregar nueva fila con versión histórica), y Tipo 3 (agregar columna para valores previos). La elección del tipo depende de requisitos de negocio sobre si se debe mantener historial de cambios.\nImagina que llevas un registro de tus amigos: si uno se muda de ciudad, puedes simplemente actualizar su dirección borrando la anterior (Tipo 1), o agregar una nueva línea manteniendo su dirección anterior con fechas de vigencia (Tipo 2), o agregar una columna \u0026ldquo;dirección_anterior\u0026rdquo; (Tipo 3).\nUna empresa rastrea información de clientes: Juan Pérez vive en Ciudad A desde 2020. En 2023 se muda a Ciudad B. Con SCD Tipo 2, se crea un nuevo registro indicando que la dirección en Ciudad A fue válida de 2020-2023 (marcando fin de vigencia), y la dirección en Ciudad B es válida desde 2023-presente. Esto permite analizar históricamente: \u0026ldquo;¿Cuántas ventas tuvimos en Ciudad A cuando Juan vivía ahí?\u0026rdquo; manteniendo precisión histórica en los reportes.\nData Maturity Es el nivel de desarrollo y sofisticación de las capacidades de gestión de datos de una organización. Se refiere a qué tan bien una empresa recopila, almacena, analiza y utiliza sus datos para la toma de decisiones. La madurez se evalúa generalmente en niveles progresivos, desde inicial hasta optimizado.\nPiensa en aprender a cocinar: al principio sigues recetas básicas sin entender por qué (nivel inicial), luego experimentas y mejoras técnicas (nivel intermedio), y finalmente creas tus propios platillos optimizando sabores (nivel avanzado).\nUna startup podría comenzar guardando datos en hojas de cálculo sin análisis formal, mientras que empresas maduras como Amazon tienen sistemas automatizados que predicen demanda y optimizan inventarios en tiempo real basándose en datos históricos y patrones complejos.\nData Pipeline Se trata de una serie de procesos digitales que se utilizan para recopilar, modificar y entregar datos de un lugar a otro. Consiste en ingerir datos sin procesar de diversas fuentes, como aplicaciones, dispositivos y otros canales digitales, y trasladarlos a un repositorio de datos, como un Data Lake o un Data Warehouse, para su análisis.\nPiensa que estás en una tienda en línea. Cuando realizas un pedido, el sitio necesita procesar su pedido, verificar el inventario, generar una etiqueta de envío y enviarle un correo electrónico de confirmación. Todos estos pasos son parte de un data pipeline porque el sitio toma su pedido, pasa por varias etapas y, finalmente, recibe su correo electrónico de confirmación.\nETL Es el proceso de Extraer (Extract), Transformar (Transform) y Cargar (Load) datos desde fuentes originales hacia un destino como un data warehouse. La extracción obtiene datos de diversos sistemas, la transformación los limpia y adapta al formato deseado, y la carga los almacena en el sistema objetivo. Es uno de los procesos más tradicionales y fundamentales en gestión de datos.\nImagina que eres chef y necesitas preparar una ensalada: primero extraes ingredientes del refrigerador (Extract), luego los lavas, cortas y mezclas según tu receta (Transform), y finalmente los sirves en un plato (Load).\nEn el mundo empresarial, un banco extrae transacciones diarias de múltiples sucursales y cajeros automáticos, las transforma para estandarizar formatos de moneda y fechas eliminando duplicados, y finalmente las carga en un data warehouse central donde los analistas pueden generar reportes consolidados de operaciones del día.\nELT Es el proceso de Extraer (Extract), Cargar (Load) y Transformar (Transform) datos, donde a diferencia del ETL tradicional, los datos se cargan directamente en el destino en su forma cruda y se transforman después usando el poder de procesamiento del sistema objetivo. Es más rápido y flexible que ETL, especialmente para grandes volúmenes de datos y cuando se trabaja con data warehouses modernos en la nube.​\nImagina que estás mudándote de casa: con ETL sería organizar y empacar perfectamente todas tus cosas antes de transportarlas, con ELT es llevar todo tal como está a la nueva casa y organizarlo allá donde tienes más espacio y herramientas. En el mundo real, una empresa de comercio electrónico recibe millones de registros de clickstream diarios.\n*Con ELT, cargan todos esos datos crudos directamente en su data warehouse en la nube (como Snowflake o BigQuery), y luego diferentes equipos transforman los datos según sus necesidades específicas: marketing extrae métricas de conversión, producto analiza patrones de navegación, todo usando el poder de procesamiento masivo del warehouse sin necesidad de servidores intermedios.​\nEtLT Es un enfoque híbrido que combina Extract, transform (lite), Load, Transform, realizando una transformación ligera inicial para manejar datos sensibles o cumplir requisitos de seguridad, luego carga los datos en el destino donde se realizan transformaciones complejas finales. Resuelve las limitaciones de seguridad del ELT puro manteniendo velocidad y eficiencia.​\nImagina que trabajas en un hospital mudando archivos médicos: primero remueves información sensible como números de seguro social (transform lite), luego transportas todo a la nueva ubicación (load), y finalmente organizas y procesas la información completa ya en el destino (transform).\n*Una empresa de salud necesita analizar millones de registros médicos rápidamente pero debe cumplir regulaciones estrictas. Con EtLT, primero extraen los datos y realizan transformaciones rápidas para enmascarar información personal identificable (PII) como nombres y direcciones, cargan estos datos anonimizados en su data warehouse, y ahí realizan análisis complejos integrando múltiples fuentes para identificar patrones de enfermedades sin comprometer la privacidad de pacientes.​\nIdempotency Es la propiedad de una operación o proceso que puede ejecutarse múltiples veces produciendo siempre el mismo resultado, sin importar cuántas veces se repita. En ingeniería de datos, garantiza que si un pipeline falla y se reintenta, los datos no se duplican ni corrompen. Es fundamental para construir sistemas confiables y resilientes que manejen errores de forma segura.​\nImagina que enciendes un interruptor de luz: presionarlo una vez enciende la luz, presionarlo 10 veces más no hace que la luz se vuelva \u0026ldquo;más encendida\u0026rdquo;, el resultado es el mismo.\n*En datos, un banco procesa transferencias diarias mediante un pipeline. Si el pipeline falla a mitad de ejecución y se reintenta, una operación idempotente asegura que la misma transferencia no se procese dos veces. Por ejemplo, usar \u0026ldquo;UPSERT\u0026rdquo; (actualizar si existe, insertar si no) en lugar de \u0026ldquo;INSERT\u0026rdquo; simple, o nombrar archivos con fechas únicas para que sobrescriban en lugar de duplicar. Esto permite que plataformas como Uber o Netflix reinten operaciones fallidas confiadamente sin crear registros duplicados o métricas incorrectas.​\nIncremental Load Es una técnica de carga de datos que transfiere únicamente los registros nuevos o modificados desde la última ejecución, en lugar de recargar todo el dataset. Utiliza mecanismos como Change Data Capture (CDC), timestamps o flags de actualización para identificar cambios. Es más rápido y eficiente que full load, permitiendo actualizaciones frecuentes con menor consumo de recursos.​\nImagina que llevas un diario: en lugar de reescribir todo el diario cada día, simplemente agregas la nueva página del día actual. Una empresa de e-commerce con millones de productos actualiza su catálogo constantemente.\n*Con incremental load, cada hora el pipeline identifica solo los productos que cambiaron de precio, se agregaron o se marcaron como agotados en esa hora (usando un campo \u0026ldquo;última_modificación\u0026rdquo;), y actualiza únicamente esos registros en el data warehouse. Si solo 5,000 productos de 10 millones cambiaron, el pipeline procesa 5,000 en lugar de recargar 10 millones completos, reduciendo el tiempo de carga de horas a minutos y permitiendo análisis casi en tiempo real.​\nFull Load Es una técnica de carga de datos que transfiere el dataset completo desde la fuente al destino, sobrescribiendo toda la información existente. Garantiza consistencia total de datos y es simple de implementar, pero consume más recursos y tiempo. Se usa típicamente en cargas iniciales, cuando no hay mecanismo para detectar cambios, o en datasets pequeños.​\nImagina que estás haciendo respaldo de tu teléfono: en lugar de identificar qué fotos son nuevas, simplemente copias todas las fotos cada vez. Una pequeña startup que genera reportes mensuales de 50,000 registros de ventas hace un full load: cada inicio de mes, elimina la tabla completa del data warehouse y recarga todos los datos desde cero.\n*Aunque no es la técnica más eficiente, garantiza que no haya inconsistencias y es simple de mantener. También se usa en la carga inicial cuando implementas un nuevo data warehouse o cuando ocurre un error crítico y necesitas \u0026ldquo;empezar de cero\u0026rdquo; para garantizar integridad total de datos.​\nCDC (Change Data Capture) Es una técnica que captura y rastrea cambios (inserciones, actualizaciones, eliminaciones) realizados en una base de datos en tiempo real o casi real, permitiendo replicar o sincronizar estos cambios hacia otros sistemas sin necesidad de recargar todo el dataset. Utiliza logs de transacciones o triggers para identificar modificaciones desde la última captura, facilitando pipelines eficientes de integración de datos.​\nImagina que tienes un cuaderno donde anotas todas las correcciones que haces a tus apuntes: página 5 cambié \u0026ldquo;rojo\u0026rdquo; por \u0026ldquo;azul\u0026rdquo;, página 10 agregué un párrafo, página 3 borré una línea. En lugar de reescribir todo el cuaderno, solo compartes esa lista de cambios. Una empresa de e-commerce tiene una base de datos transaccional donde se actualizan precios, inventarios y órdenes constantemente.\n*Con CDC, cada cambio en la base de datos se captura automáticamente mediante los logs de transacciones (como el binlog en MySQL) y se transmite en tiempo real a sistemas analíticos o data warehouses. Si un producto cambia de precio de $100 a $80, CDC captura ese cambio específico y lo replica, sin necesidad de leer toda la tabla de productos cada vez.​\nDead Letter Queue (DLQ) Es un mecanismo de almacenamiento temporal para mensajes o registros que no pueden ser procesados exitosamente por un pipeline después de múltiples reintentos. En lugar de perder estos datos o detener el pipeline completo, los registros problemáticos se envían a una cola especial donde pueden ser investigados, corregidos y reprocesados posteriormente sin afectar el flujo principal de datos.\nImagina una línea de producción de una fábrica: si una pieza está defectuosa y no puede pasar por el proceso normal, en lugar de detener toda la línea, la apartan en una bandeja especial para revisión posterior.\nUn pipeline de ingesta de eventos de comercio electrónico procesa millones de transacciones diarias. Ocasionalmente llegan registros con formatos inesperados que causan errores. En lugar de fallar todo el pipeline, estos registros se envían automáticamente a una DLQ en Amazon SQS. El equipo revisa semanalmente la DLQ, identifica patrones de errores, corrige el código de validación, y reprocesa los registros corregidos sin haber perdido ninguna transacción ni interrumpido el servicio.\nData Deduplication Es el proceso de identificar y eliminar registros duplicados de un dataset para garantizar que cada ocurrencia se procese solo una vez. En pipelines batch, la deduplicación analiza el dataset completo; en pipelines streaming, utiliza ventanas de tiempo para mantener un estado de registros ya procesados. Requiere definir atributos de deduplicación que garanticen la unicidad de cada registro.​\nImagina que tienes un cuaderno donde anotas tus gastos diarios, pero a veces escribes la misma compra dos veces por error. La deduplicación sería revisar tu cuaderno y eliminar las entradas repetidas dejando solo una.\n*Una plataforma de streaming como Netflix recibe eventos de visualización de usuarios que, debido a reintentos automáticos de la red, pueden llegar duplicados. Un sistema de deduplicación identifica eventos con el mismo ID de usuario, video y timestamp, retiene solo el primero, y descarta los duplicados, asegurando que las métricas de visualizaciones sean precisas. Para streaming, el sistema mantiene una ventana temporal (por ejemplo, últimos 10 minutos) donde recuerda IDs ya procesados, balanceando precisión con uso de recursos.​\nData Compaction Es el proceso de optimizar el almacenamiento combinando múltiples archivos pequeños en archivos más grandes para mejorar el rendimiento de lectura y reducir overhead de metadatos. Es especialmente crítico en sistemas que escriben datos incrementalmente o en streaming, donde se generan miles de archivos pequeños que degradan el rendimiento de consultas. No modifica los datos, solo reorganiza su disposición física.​\nImagina que guardas documentos importantes: en lugar de tener 100 carpetas con una sola hoja cada una, es mejor consolidarlas en 5 carpetas con 20 hojas cada una, facilitando encontrar información.\n*Un pipeline de streaming procesa eventos de IoT cada minuto, generando 1,440 archivos pequeños diarios (uno por minuto). Después de un mes, acumula 43,200 archivos diminutos. Consultar este dataset se vuelve extremadamente lento porque el sistema debe abrir miles de archivos. Un proceso de compactación periódico (por ejemplo, cada hora) combina esos 60 archivos pequeños en uno solo más grande, reduciendo de 43,200 a 720 archivos mensuales, mejorando dramáticamente la velocidad de consultas sin perder información.​\nQuery Es una solicitud o consulta que se realiza a una base de datos para recuperar, manipular o actualizar información específica. Las queries se escriben típicamente en lenguajes especializados como SQL (Structured Query Language) y permiten filtrar, ordenar, agregar y transformar datos según las necesidades del usuario.\nImagina que estás en una biblioteca enorme y le dices al bibliotecario exactamente qué libro quieres: \u0026ldquo;Dame todos los libros de ciencia ficción publicados después del 2020\u0026rdquo;. Eso es una query.\nEn datos, si trabajas en una tienda de comercio electrónico y necesitas saber cuántos productos se vendieron ayer con un precio mayor a $100 en la región norte, escribirías una query que busca esa información específica en la base de datos y te devuelve exactamente esos resultados en segundos.\nData Contract Es un documento que define la estructura, el formato, la semántica, la calidad y los términos de uso para el intercambio de datos entre un proveedor de datos y sus consumidores. Ayuda a garantizar que los datos sean coherentes, confiables y comprensibles en diferentes sistemas.\nTu eres un chef que necesita otros ingredientes de un proveedor. En este caso, un contrato de datos sería una lista de compras detallada que especifica claramente el tipo de ingredientes, la cantidad necesaria, etc.\nAhora bien, en el campo de los datos, diferentes sistemas necesitan compartir o intercambiar datos. Para garantizar una comunicación fluida, un contrato ayuda a definir la estructura y las reglas para los datos que se comparten. Especifica aspectos como el formato de los datos (por ejemplo, CSV, JSON), los campos y sus tipos, cualquier regla de validación o restricción y el comportamiento esperado.\nData Entropy Describe la cantidad de incertidumbre o desorden en un conjunto de datos. Cuanto mayor sea la entropía, mayor será la aleatoriedad y la falta de patrones en los datos.\nTienes una baraja de cartas que está perfectamente ordenada del as al rey en cada palo. En este caso, la entropía de los datos es baja porque el orden es predecible y no contiene mucha aleatoriedad. Ahora, consideremos una baraja de cartas barajada donde las cartas están en un orden aleatorio. En este caso, la entropía de los datos es alta porque el orden es impredecible y contiene más aleatoriedad.\nData Debt Es la acumulación de problemas que surgen de prácticas inadecuadas de gestión de datos. Similar a la deuda técnica, resulta de descuidar el mantenimiento de activos de datos, generando inconsistencias, redundancias e imprecisiones. Con el tiempo, esta deuda se vuelve costosa de resolver y afecta la confiabilidad de las decisiones.\nImagina que empiezas guardando recetas en servilletas, post-its y cuadernos diferentes sin orden. Al principio funciona, pero después de meses no encuentras las recetas y tienes versiones contradictorias de la misma.\nEn el ámbito empresarial, un equipo de ciencia de datos presionado por entregar resultados rápidos decide acceder directamente a bases de datos de origen sin estándares ni mejores prácticas, creando pipelines improvisados. Con el tiempo acumulan múltiples versiones de la misma métrica, nadie sabe cuál es correcta, y los costos de mantenimiento se disparan.\nData Silo Los silos de datos son una colección de datos que está controlada por un departamento o unidad de negocios y aislada del resto de la organización. Normalmente, los datos terminan almacenándose en un sistema separado y, a menudo, son incompatibles con otros conjuntos de datos, lo que dificulta que los usuarios de otras partes de la organización accedan a ellos y los utilicen.\nImagina que tienes varias piezas de rompecabezas esparcidas en diferentes habitaciones de tu casa. Cada habitación representa un departamento diferente dentro de una empresa y las piezas del rompecabezas representan datos.\nEn el escenario del silo de datos, cada departamento tiene su propia pieza del rompecabezas que está separada de las demás. Las piezas de una habitación no son accesibles ni compartidas con otras habitaciones. Esto significa que cada departamento tiene su propio conjunto de datos que está aislado del resto de la organización.\nData Virtualization Es una tecnología que integra datos de múltiples fuentes creando una capa virtual unificada sin necesidad de mover o copiar físicamente la información. Permite a usuarios y aplicaciones acceder a datos en tiempo real desde su ubicación original, abstrayendo la complejidad técnica de dónde y cómo están almacenados. Incluye funcionalidades avanzadas como cacheo, seguridad y optimización de consultas.\nImagina que tienes fotos en tu teléfono, computadora y en la nube, pero usas una aplicación que te muestra todas juntas como si estuvieran en un solo lugar sin tener que copiarlas. En una empresa de retail, los datos de ventas están en un sistema, inventario en otro, y datos de clientes en la nube.\nCon virtualización, los analistas consultan todo desde una interfaz única que presenta los datos como si estuvieran en un solo lugar, obteniendo información en tiempo real sin crear copias costosas ni procesos ETL complejos.\nData Federation Es una técnica específica de integración que permite consultar y acceder a múltiples bases de datos distribuidas como si fueran una sola fuente unificada. Los datos permanecen en sus sistemas originales sin moverse, y las consultas se traducen y ejecutan en cada fuente, agregando los resultados de manera transparente para el usuario. Es un componente de data virtualization enfocado en bases de datos.\nImagina una red de bibliotecas públicas donde cada sucursal mantiene sus propios libros, pero tú puedes buscar en un catálogo unificado y solicitar cualquier libro de cualquier sucursal sin necesidad de visitarlas todas. Una empresa multinacional con oficinas en diferentes países mantiene bases de datos locales para cumplir regulaciones regionales.\nCon data federation, los ejecutivos globales pueden ejecutar reportes que consultan automáticamente todas las bases regionales simultáneamente, obteniendo resultados consolidados sin centralizar físicamente los datos ni violar regulaciones de residencia de datos.\nData Management Es el proceso de recopilar, almacenar, organizar y utilizar datos de una manera segura, eficiente y rentable.\nTienes una gran colección de fotos familiares almacenadas en tu computadora. Para administrar mejor tu colección, crea carpetas y subcarpetas para categorizar las fotos según eventos (probablemente separa las carpetas por cumpleaños, vacaciones, etc.). Si deseas encontrar una foto en particular, es mucho más fácil navegar hasta la carpeta correspondiente en lugar de buscar las fotos una por una.\nDe manera similar, en la administración de datos, los datos deben organizarse, etiquetarse y almacenarse en sistemas apropiados. Esto lleva a definir estructuras de datos, establecer convenciones de nomenclatura de datos, determinar controles de acceso e implementar mecanismos de copia de seguridad y recuperación de datos.\nMaster Data Management Es un proceso y un conjunto de prácticas destinadas a crear y gestionar un único \u0026lsquo;golden record\u0026rsquo; de entidades de datos importantes dentro de una organización para garantizar la coherencia, la precisión y la fiabilidad. Un MDM proporciona una visión unificada de los datos en varios sistemas para satisfacer las necesidades de una empresa.\nFormas parte de una empresa minorista que opera varias tiendas y una plataforma en línea. En esta empresa, tienes datos de clientes dispersos en diferentes sistemas y bases de datos (como registros de ventas, programas de fidelización y registros en línea). Sin una gestión de datos maestros adecuada, puedes terminar teniendo registros duplicados o inconsistencias en la entidad del cliente (por ejemplo, tiene a John Smith, y en el sistema de ventas tiene diferentes entradas, programa de fidelidad y sistema de registro en línea con diferentes variaciones en el nombre, información de contacto, etc.)\nEntonces, la empresa decide abordar este problema mediante la creación de un MDM. Decide crear un repositorio central que actúe como la única fuente de verdad para los datos de los clientes. En este sistema de gestión de datos maestros, se consolidan, estandarizan y eliminan los duplicados de distintas fuentes. De esta manera, en lugar de tener múltiples versiones de los registros de \u0026lsquo;Juan Hernandez\u0026rsquo;, el MDM garantiza que solo exista un registro consolidado y preciso.\nData Democratization Significa que todos en la organización pueden acceder, comprender y usar los datos para tomar decisiones sin depender exclusivamente de especialistas en datos o departamentos de TI. Elimina los silos de datos y promueve la colaboración entre diferentes usuarios, empoderando a los equipos con autoservicio analítico.\nImagina una biblioteca donde antes tenías que pedirle al bibliotecario cada libro que querías consultar, y él decidía si te lo daba. Democratización sería que ahora puedes entrar, buscar y tomar los libros tú mismo.\nUna empresa de comercio electrónico tenía un equipo de análisis centralizado donde marketing, ventas y operaciones debían solicitar e interpretar datos. Al implementar democratización, introdujeron herramientas de autoservicio con interfaces fáciles donde cada departamento ahora puede generar sus propios reportes y análisis sin involucrar al equipo de análisis en cada paso.\nData Catalog Es un inventario organizado de activos de datos que utiliza metadatos para ayudar a una organización a administrar sus datos. Piense en él como un repositorio centralizado donde puede encontrar información relevante para sus necesidades de datos, ya que lo ayuda a comprender qué datos están disponibles, dónde se encuentran y cómo puede acceder a ellos.\nEstás en una tienda minorista. Un catálogo de datos tendría información de varias fuentes de datos, como datos de ventas, datos de clientes, datos de inventario, etc. Esto tendría detalles como qué conjunto de datos tienen, cuándo se actualizó por última vez, quién lo administra y metadatos relevantes.\nUn catálogo de datos abarca una gama más amplia de información sobre varios activos de datos en toda la organización, incluidos metadatos, linaje de datos, calidad de datos e información de acceso. El objetivo es proporcionar una vista integral del panorama de datos de la organización.\nData Dictionary Se centra en proporcionar definiciones y descripciones de elementos de datos específicos dentro de una base de datos o conjunto de datos. Le ayuda a comprender el significado y el formato de los elementos de datos individuales.\nTienes una aplicación de gestión de contactos y desea almacenar información sobre tus amigos. Para cada amigo, deseas almacenar su nombre, número de teléfono y dirección de correo electrónico.\nUn diccionario de datos te ayudaría a obtener una descripción general de los datos disponibles, te ayudaría a identificar recursos relevantes y te permitiría ver detalles técnicos como esquemas, formatos de datos, mantenedores, etc.\nData Ops Es la aplicación de prácticas de DevOps (desarrollo y operaciones) al ciclo de vida de los datos, enfocándose en mejorar la colaboración, automatización, monitoreo y calidad de los pipelines de datos. Combina metodologías ágiles, control de versiones, integración continua, testing automatizado y observabilidad para entregar datos confiables más rápidamente y con menos errores.\nImagina una fábrica moderna con líneas de producción automatizadas, sensores que detectan defectos, y sistemas que alertan inmediatamente si algo sale mal, en lugar de una fábrica antigua donde todo se hace manualmente y los problemas se descubren días después.\nUn equipo de data engineering implementa DataOps cuando versiona sus pipelines en Git (como código), ejecuta tests automatizados antes de desplegar cambios en producción, monitorea la calidad de datos en tiempo real con alertas cuando aparecen anomalías, y puede revertir cambios problemáticos en minutos. Esto contrasta con equipos que modifican pipelines manualmente, descubren errores semanas después cuando usuarios reportan números incorrectos, y tardan días en identificar qué salió mal.\nData Orchestration Es el proceso coordinado de automatizar, programar y gestionar múltiples tareas y flujos de trabajo de datos para que se ejecuten en el orden correcto, con las dependencias apropiadas y en el momento adecuado. Actúa como el director de una orquesta que asegura que cada instrumento (pipeline, transformación, validación) toque en el momento preciso para crear una sinfonía armoniosa de flujo de datos.\nImagina organizar una cena grande: primero compras ingredientes, luego preparas entradas mientras el plato principal se cocina, y finalmente sirves el postre cuando todos terminan. No puedes servir el postre antes de las entradas. La orquestación de datos funciona igual: coordina tareas secuenciales y paralelas.\nUna empresa de e-commerce ejecuta diariamente: extraer datos de ventas a las 2 AM, transformarlos a las 3 AM (después de la extracción), cargar métricas a las 4 AM, y finalmente enviar reportes ejecutivos a las 6 AM. Herramientas como Apache Airflow o Prefect automatizan esta coordinación, reintentando tareas fallidas, enviando alertas cuando algo falla, y asegurando que cada paso espere a que el anterior complete exitosamente antes de ejecutarse.\nData Governance Son las políticas, reglas y prácticas que garantizan la calidad, integridad y seguridad de los datos dentro de una organización. Incluye la catalogación de datos, la definición de estándares, y los procesos que regulan cómo se utilizan, acceden y mantienen los datos.\nImagina una biblioteca pública bien organizada: hay reglas sobre quién puede sacar libros, cuánto tiempo pueden tenerlos, y cómo se catalogan.\nEn el mundo empresarial, un hospital implementa gobernanza cuando establece quién puede acceder a los registros médicos, cómo se protegen, qué información es confiable para decisiones médicas, y quién es responsable si surgen problemas con los datos de pacientes.\nData Owner El individuo o entidad que tiene la responsabilidad y el control final sobre activos de datos específicos. El propietario de los datos suele ser responsable de determinar quién tiene acceso a los datos, garantizar su precisión y seguridad, y definir su uso permitido.\nUn ejemplo podría ser un hospital, donde el médico jefe o el administrador del hospital pueden ser designados como el propietario de los datos de los registros médicos de los pacientes. Serían responsables de supervisar quién puede acceder a los registros, mantener su confidencialidad y garantizar el cumplimiento de las normas de protección de datos.\nData Steward Se trata de una persona responsable de gestionar y garantizar la calidad, la seguridad y el uso de los activos de datos de una organización. Por lo general, establece y aplica políticas y procedimientos de gestión de datos, supervisa la integración de datos y facilita el cumplimiento normativo.\nEres la persona a cargo de una institución financiera que supervisa la protección y privacidad de los datos de los clientes. Es responsable de garantizar que los datos de los clientes se gestionen de acuerdo con los requisitos legales, los estándares de la industria y las políticas internas, actuando así como un administrador de datos para los datos financieros confidenciales de la organización.\nData Guardian Hace referencia a una función, política o tecnología específicamente designada para proteger la integridad, confidencialidad y disponibilidad de los datos. Esto podría incluir la gestión de permisos, la implementación de medidas de seguridad y el control del acceso a los datos.\nImagina que has dejado tu casa al cuidado de un vecino de confianza mientras estás de vacaciones. Este vecino vigila tu casa, riega tus plantas y se asegura de que no entren visitantes no deseados. En este escenario, tu casa y tus pertenencias son tus datos, y el vecino es el guardián de los datos que mantiene todo seguro y en orden hasta que regreses.\nEn un entorno de datos, un guardián de datos es crucial. Un guardián supervisaría los registros de los pacientes, los tipos de datos sensibles que requieren una protección rigurosa. El guardián se aseguraría de que los datos médicos estén encriptados, de que el acceso se registre y analice para detectar actividades no autorizadas, y de que los datos se compartan de forma segura con las partes autorizadas.\nData Security Se refiere a la protección de los datos digitales contra el acceso no autorizado, la corrupción o el robo a lo largo de su ciclo de vida. Implica la implementación de medidas como el cifrado, los controles de acceso y la supervisión para salvaguardar la información confidencial y evitar infracciones o divulgaciones no autorizadas.\nUna institución financiera cifra los datos financieros de los clientes e implementa controles de acceso estrictos para evitar que personas no autorizadas vean o modifiquen los datos. Esto ayuda a proteger la información financiera confidencial de los clientes de las amenazas cibernéticas y las posibles infracciones de datos.\nData Privacy Se trata de respetar los derechos y preferencias de las personas en relación con el uso y el manejo de sus datos personales. Es el manejo responsable de la información personal de las personas, garantizando que sus datos estén protegidos contra el acceso, uso o divulgación no autorizados.\n\u0026ldquo;No necesitas privacidad si no tienes nada que ocultar\u0026rdquo;. Esta es una mala manera de interpretar la privacidad porque crea la sensación de que las personas que exigen privacidad deben ser delincuentes. Todos sabemos lo que pasa cuando vas a bañarte pero aún así cierras la puerta. Un ejemplo de privacidad de datos es cuando un minorista en línea recopila información personal de los clientes para procesar pedidos, pero garantiza que estos datos se almacenan de forma segura y que se obtiene el consentimiento de los clientes para las comunicaciones de marketing.\nData Lifecycle Se refiere a las etapas por las que pasa la información desde su creación o captura inicial hasta su eliminación o archivo final. Estas etapas suelen incluir la creación de datos, el almacenamiento, el uso, el intercambio, el archivo y la eliminación.\nEs como el recorrido de un libro: desde que el autor lo escribe, pasando por su publicación, la lectura por parte de la gente, el almacenamiento en una biblioteca y, posiblemente, el desmantelamiento.\nEn el ámbito de los datos, un ejemplo del ciclo de vida de los datos sería la información de productos de una empresa minorista. Comienza con la creación de la información del producto, luego se almacena en una base de datos, se utiliza para las ventas en línea, se comparte con los proveedores, se archiva para el análisis histórico y, finalmente, se elimina cuando el producto ya no está disponible.\nData Engineering Lifecycle El ciclo de ingeniería de datos implica la recopilación, el almacenamiento, el procesamiento, el análisis y el mantenimiento de la infraestructura. Se descubren las fuentes, se define el almacenamiento, se define la ingesta, se transforma y, finalmente, se pone a disposición la información.\nUna empresa de comercio electrónico ingiere datos de múltiples fuentes, los transforma, los integra, realiza análisis y visualiza la información para tomar mejores decisiones. Es un proceso iterativo e implica un seguimiento y una mejora continuos.\nData Sources Se refiere a la fuente o ubicación de la que se recopilan o extraen datos para su uso en análisis, informes o toma de decisiones.\nLas fuentes de datos se pueden comparar con los diferentes ingredientes que se utilizan en la cocina, como frutas, verduras y especias, que se recopilan de varias ubicaciones para crear una receta.\nEn el mundo de los datos, un ejemplo de fuentes de datos es una empresa que recopila información de sistemas dispares, como transacciones de ventas de un sistema de punto de venta, datos de clientes de una plataforma CRM y datos de tráfico web de una herramienta de análisis, para el análisis y la elaboración de informes comerciales.\nData Storage Es un lugar centralizado donde se recopilan y combinan datos de múltiples fuentes. Conlleva conservar los datos en un formato estructurado para acceder a ellos y utilizarlos en el futuro.\nEs como encontrar un lugar para guardar tus libros en una librería para que luego puedas encontrarlos y usarlos cuando los necesites. En el mundo digital, este concepto implica el uso de sistemas o dispositivos para almacenar y recuperar información digital.\nData Ingestion Es el proceso de recopilar, importar y transferir datos de varias fuentes a un sistema informático o de almacenamiento para su posterior procesamiento y análisis.\nEs como recopilar y organizar ingredientes de diferentes proveedores y llevarlos a la cocina de un restaurante para preparar comidas.\nAhora bien, en materia de datos, un ejemplo sería una empresa minorista que recopila datos de ventas de varias tiendas y canales en línea y los incorpora a un almacén de datos centralizado para su análisis y elaboración de informes.\nData Integration Se centra en combinar datos de distintas fuentes en una vista unificada y coherente. Su finalidad es establecer un modelo de datos común.\nDe la misma forma en que se juntan piezas de un rompecabezas de distintos lugares para completar el cuadro, la integración de datos unifica las fuentes. Un ejemplo sería una empresa que fusiona datos de clientes de un CRM, datos de ventas de un sistema ERP y datos de marketing de campañas digitales para crear una vista integral para el análisis y la toma de decisiones comerciales estratégicas.\nData Transformation Es el proceso de convertir datos de un formato, estructura o sistema a otro para que sean utilizables y compatibles con el destino final. Incluye actividades como limpiar, normalizar, agregar, filtrar o enriquecer los datos. Es una etapa crítica en cualquier pipeline de datos para asegurar que la información esté lista para el análisis o almacenamiento.\nImagina que recolectas recetas de cocina de diferentes países: unas están en tazas, otras en gramos, algunas en celsius y otras en fahrenheit. Transformar sería convertir todo a un sistema único (por ejemplo, todo a gramos y celsius) para poder comparar recetas fácilmente.\nEn el mundo real, una empresa de comercio electrónico recibe datos de ventas de múltiples tiendas con formatos diferentes: algunas usan \u0026ldquo;USD\u0026rdquo;, otras \u0026ldquo;$\u0026rdquo;, las fechas varían entre \u0026ldquo;DD/MM/YYYY\u0026rdquo; y \u0026ldquo;MM-DD-YY\u0026rdquo;. El proceso de transformación convierte todo a un formato estándar para que los analistas puedan crear reportes consolidados precisos.\nData Serving Es el proceso de hacer que los datos procesados y transformados sean accesibles y estén disponibles para los usuarios finales o aplicaciones de manera eficiente y en el formato adecuado. Implica proporcionar acceso mediante APIs, dashboards, reportes o consultas directas, asegurando que la información llegue rápidamente a quien la necesita.\nImagina que estás en un restaurante: los chefs preparan la comida en la cocina y, cuando está lista, los meseros la sirven en tu mesa de forma presentable y a la temperatura correcta. En datos, los usuarios son los clientes y los datos procesados son los platillos listos para consumir. Una empresa de logística procesa millones de datos de entregas cada día.\nEl equipo de Data Serving se encarga de que los gerentes de operaciones puedan consultar en tiempo real cuántos paquetes están en tránsito, los equipos de servicio al cliente vean el estatus de envíos específicos, y los ejecutivos accedan a dashboards con métricas clave, todo sin tocar las bases de datos originales directamente.\nStaging Data Es el proceso de almacenar y preparar temporalmente datos para cargarlos en un almacén de datos, lago de datos u otro repositorio de datos.\nEs como preparar y organizar todas las herramientas, equipos y materiales necesarios antes de comenzar un proyecto en el hogar, como pintar una habitación o ensamblar muebles. Implica tener todo listo y organizado para facilitar la ejecución fluida del proyecto.\nEn datos, sería almacenar y estructurar datos sin procesar de varias fuentes en un área de preparación antes de integrarlos en una plataforma unificada de almacenamiento o análisis.\nData Warehouse Es un repositorio centralizado diseñado específicamente para almacenar grandes volúmenes de datos estructurados e históricos provenientes de múltiples fuentes. Está optimizado para consultas analíticas complejas, generación de reportes y toma de decisiones estratégicas. A diferencia de bases de datos transaccionales, su enfoque no es procesar operaciones en tiempo real sino facilitar análisis históricos y tendencias.\nImagina que tienes una biblioteca personal donde guardas libros desordenadamente en diferentes habitaciones. Cuando quieres investigar un tema, pierdes horas buscando. Un data warehouse sería consolidar todos los libros en una biblioteca organizada con un catálogo único donde encuentras todo rápidamente.\nUna cadena de retail como Walmart recibe datos diarios de miles de tiendas (ventas, inventario, devoluciones), sitios web (clicks, carritos), proveedores (entregas, precios) y redes sociales (menciones). El data warehouse centraliza y organiza toda esta información histórica en un formato optimizado donde analistas y ejecutivos pueden generar reportes consolidados comparando ventas de este trimestre vs años anteriores, identificar productos de bajo rendimiento, o analizar patrones de compra por región sin tocar los sistemas operacionales.\nOLAP Online Analytical Processing (OLAP) es una tecnología diseñada para realizar análisis multidimensionales complejos y consultas sobre grandes volúmenes de datos históricos. Está optimizado para lectura, agregaciones rápidas y análisis de tendencias, soportando operaciones como drill-down, slice-and-dice y pivot. Es la base de Business Intelligence y reportes analíticos.\nImagina un cubo Rubik donde cada cara representa una dimensión de análisis: puedes girar y ver datos de ventas por región, luego por producto, luego por mes, todo instantáneamente.\nUna cadena de supermercados usa OLAP para analizar ventas: los ejecutivos pueden ver ventas totales del año, hacer drill-down a un trimestre específico, filtrar por categoría de productos, comparar regiones, y pivotar para ver todo por línea de tiempo o por tienda, todo en segundos sin esperar que se procesen millones de transacciones porque OLAP ya tiene los datos precalculados y organizados para análisis rápidos.\nOLTP Online Transaction Processing (OLTP) es un sistema diseñado para gestionar y procesar transacciones operacionales en tiempo real con alta velocidad y concurrencia. Está optimizado para operaciones de escritura frecuentes, consultas simples y rápidas, garantizando integridad de datos mediante propiedades ACID. Maneja las operaciones diarias del negocio como ventas, reservas o actualizaciones de inventario.\nImagina la caja registradora de una tienda: necesita procesar cada compra instantáneamente, actualizar inventario, registrar pago, y garantizar que todo sea preciso sin errores. Eso es OLTP.\nCuando compras un boleto de avión en línea, el sistema OLTP procesa tu reservación en segundos: verifica disponibilidad de asiento, bloquea ese asiento para que nadie más lo tome, registra tu pago, actualiza inventario, y confirma tu compra, todo garantizando que si dos personas intentan reservar el mismo asiento simultáneamente, solo una lo obtenga. Este tipo de sistema maneja miles de transacciones concurrentes por segundo con precisión absoluta.\nData Mart Es un subconjunto del Data Warehouse de una organización que está diseñado para servir a una línea de negocio o departamento específico.\nEs como una sección especializada en una biblioteca que contiene libros, revistas y recursos enfocados en un tema o asunto específico, satisfaciendo las necesidades de un grupo particular de lectores.\nSi ponemos el tema de datos en contexto, un ejemplo práctico de un Data Mart es un departamento de ventas que tiene su propio Data Mart dentro del Data Warehouse de la empresa, dedicado a almacenar y analizar datos relacionados con las ventas para los requisitos específicos de análisis e informes del departamento.\nData Lake Es un repositorio que puede recopilar una gran cantidad de datos estructurados, semiestructurados y no estructurados que se almacenan hasta que se necesitan para su procesamiento o análisis.\nVas de viaje a una playa y tienes varias fotos de dónde estuviste. En lugar de organizar tus fotos, las envías a tu Data Lake donde estarán disponibles en su estado original. Cuando quieras clasificarlas, puedes elegir qué fotos y organizarlas según tus necesidades.\nData Lakehouse Es una arquitectura moderna de gestión de datos que combina elementos de un Data Lake y un Data Warehouse. Permite almacenar grandes volúmenes de datos estructurados y no estructurados como un lago, pero también soporta consultas y análisis eficientes típicos de un almacén. Unifica flexibilidad y rendimiento en una sola plataforma.\nImagina que tienes una biblioteca donde algunos libros están organizados por categoría (estructurados) y otros están en cajas sin ordenar (no estructurados). Un lakehouse sería como tener ambos en el mismo edificio con un sistema que te permite buscar rápidamente en ambos tipos.\nNetflix maneja datos masivos de usuarios, preferencias, streaming y metadata de contenido. Con un lakehouse, pueden almacenar todo en un solo lugar y realizar análisis complejos para recomendar películas y optimizar la calidad del streaming sin mover constantemente los datos entre diferentes sistemas.\nData Platform Es una infraestructura tecnológica que permite la recopilación, el almacenamiento, la gestión y el análisis de datos de diversas fuentes para respaldar las operaciones comerciales y la toma de decisiones.\nUna plataforma de datos se asemeja a un panel de control central que reúne varias herramientas y sistemas, lo que permite a los usuarios acceder, administrar y analizar los datos de manera eficaz, como un único panel para múltiples funciones.\nUna plataforma de datos agiliza el proceso de recopilación, gestión y almacenamiento de datos, haciéndolos accesibles y utilizables para una variedad de aplicaciones. Proporciona gestión de datos en toda la extensión del entorno, incluidas funciones críticas para el negocio, como la seguridad y la observabilidad.\nSin una plataforma de datos, cada componente suele ser manejado por una herramienta o conjunto de herramientas diferente para hacer que los datos fluyan desde la fuente hasta el usuario final en un entorno complejo.\nData Fabric Es una arquitectura unificada que proporciona acceso, gestión y gobernanza consistente de datos a través de toda la organización mediante una \u0026ldquo;tela\u0026rdquo; o red que conecta diversas fuentes de datos. Permite integrar, transformar y compartir datos sin problemas independientemente de su ubicación, automatizando muchas tareas de gestión de datos mediante inteligencia artificial y metadatos.​\nImagina una red eléctrica de una ciudad: no importa si la electricidad viene de paneles solares, una represa o carbón, todos están conectados a una red única que distribuye energía a cualquier hogar que la necesite de forma transparente.\n*Una empresa multinacional tiene datos en bases de datos locales, aplicaciones en la nube, sistemas legacy y data lakes. Con data fabric, crean una capa inteligente que conecta automáticamente todas estas fuentes, permitiendo que un analista en México consulte datos de servidores en Europa y Asia sin saber siquiera dónde están físicamente, todo con gobernanza centralizada y transformaciones automatizadas.​\nData Mesh Es un enfoque descentralizado de arquitectura de datos que distribuye la propiedad, el acceso y la gobernanza de los datos entre diferentes dominios o unidades de negocio dentro de una organización. En lugar de tener un equipo centralizado que controla todos los datos, cada área es responsable de sus propios datos como productos independientes pero colaborativos.\nImagina una ciudad donde en lugar de tener una sola biblioteca gigante controlada centralmente, cada barrio tiene su propia biblioteca que administra sus propios libros, pero todas comparten un catálogo común y reglas similares de préstamo. Si necesitas un libro de otro barrio, puedes solicitarlo fácilmente.\nEn una empresa grande como Spotify, en lugar de que un solo equipo de datos centralizado gestione toda la información, el equipo de podcasts administra sus datos, el equipo de música los suyos, y el equipo de usuarios los propios, pero todos colaboran usando estándares compartidos para que los datos fluyan entre áreas cuando sea necesario.\nData Sharing Es el proceso de poner los mismos recursos de datos a disposición de múltiples aplicaciones, usuarios u organizaciones. Conlleva tecnologías, prácticas, marcos legales y elementos culturales que facilitan el acceso seguro a los datos para múltiples entidades sin comprometer la integridad de los datos.\nData Sharing mejora la eficiencia dentro de una organización y fomenta la colaboración con proveedores y socios. Permite a las partes interesadas aprender unas de otras y colaborar en prioridades compartidas.\nData Sharing puede ir desde artículos de investigación o publicaciones académicas hasta estadísticas corporativas, datos científicos o revisiones anuales de desempeño.\nData Product Es una aplicación o herramienta de software que utiliza datos para brindar información, servicios o funcionalidades valiosas a los usuarios u otros sistemas.\nEs como una aplicación de un smartphone que utiliza datos de ubicación para ofrecer recomendaciones personalizadas de restaurantes cercanos, lo que ayuda a los usuarios a tomar decisiones informadas sobre dónde cenar.\nUn producto de datos es un dashboard de business intelligence que integra y visualiza datos de ventas, marketing y finanzas para brindar información útil a los tomadores de decisiones dentro de una organización.\nData Quality Abarca dimensiones como la precisión, la integridad, la coherencia, la fiabilidad y la puntualidad. Implica procesos y tecnologías que miden, gestionan y mejoran la salud de los datos. Mantener la calidad de los datos requiere vigilancia en las prácticas de gestión de datos y un seguimiento constante para detectar y corregir problemas.\nPiensa como si estuvieras haciendo un viaje por carretera utilizando un mapa. Si el mapa está actualizado, es preciso y detallado, es probable que el viaje sea tranquilo, pero si está desactualizado, puede perderse o retrasarse. Los datos de alta calidad son como un mapa preciso y actualizado para una empresa, que conduce a mejores decisiones y operaciones más eficientes.\nData Observability Es la capacidad de comprender completamente el estado de salud de los datos en un sistema mediante monitoreo, alertas y análisis de métricas clave como frescura, volumen, esquema, linaje y distribución. Permite detectar, diagnosticar y resolver problemas de calidad de datos proactivamente antes de que afecten decisiones de negocio o análisis.\nImagina que eres médico monitoreando la salud de un paciente: mides signos vitales constantemente (temperatura, presión, ritmo cardíaco) y recibes alertas si algo está mal. Data observability hace lo mismo con tus datos.\nUna plataforma de streaming como Netflix monitorea constantemente sus pipelines de datos: si detectan que las métricas de visualización no se actualizaron en las últimas 2 horas cuando deberían actualizarse cada hora (problema de frescura), o si el volumen de datos recibidos cayó 50% inesperadamente, el sistema alerta automáticamente al equipo de ingeniería mostrando exactamente dónde falló el pipeline para que puedan resolverlo antes de que afecte las recomendaciones a usuarios.\nData Gathering Es el proceso de recopilar, compilar y capturar información de diversas fuentes. La recopilación es esencial para adquirir la materia prima necesaria para el análisis, la interpretación y la toma de decisiones.\nImagina que quieres saber qué sabor de helado prefiere la gente. Haces una encuesta preguntando a tus amigos y vecinos cuál es su favorito. La recopilación de datos es simplemente reunir todas esas respuestas. En el mundo empresarial, una tienda podría realizar encuestas para conocer las preferencias de los clientes sobre un nuevo producto, recopilando respuestas que luego analizará para comprender mejor las necesidades y preferencias de sus clientes.\nData Scalability Es la capacidad de un sistema de datos para crecer y manejar volúmenes crecientes de información, usuarios concurrentes o cargas de trabajo sin degradar significativamente el rendimiento. Incluye escalabilidad vertical (agregar recursos a un servidor) y horizontal (agregar más servidores). Es fundamental para sistemas que anticipan crecimiento continuo de datos.\nImagina una carretera: cuando hay poco tráfico funciona bien, pero cuando crece la ciudad necesitas expandirla. Escalabilidad vertical es hacer los carriles más anchos; horizontal es construir nuevas carreteras paralelas. Instagram comenzó con miles de usuarios compartiendo fotos, hoy tiene miles de millones.\nSu infraestructura de datos es escalable horizontalmente: cuando el volumen de fotos y usuarios crece, automáticamente agregan más servidores distribuidos globalmente que trabajan en paralelo, permitiendo que usuarios en México, Japón y España suban y vean fotos simultáneamente sin ralentizaciones, manejando petabytes de datos sin colapsar.\nData Latency Es el tiempo de retraso entre el momento en que ocurre un evento que genera datos y el momento en que esos datos están disponibles para su consulta o análisis. Puede variar desde milisegundos (baja latencia) hasta horas o días (alta latencia), dependiendo de la arquitectura y requisitos del sistema. Es crítica para aplicaciones en tiempo real.\nImagina que estás viendo un partido de fútbol: verlo en vivo en el estadio es latencia cero, verlo por TV es 5 segundos de latencia, verlo por streaming puede ser 30 segundos, y ver el resumen al día siguiente es alta latencia. Una aplicación de trading financiero requiere latencia ultra baja (milisegundos) porque cada segundo cuenta para decisiones de compra/venta de acciones.\nEn contraste, un reporte mensual de ventas puede tolerar alta latencia (horas). Uber necesita baja latencia en el matching de conductores (segundos), pero puede tener mayor latencia en reportes de tendencias mensuales que los ejecutivos revisan después.\nData Engineer Es el profesional encargado de construir, mantener y optimizar la infraestructura y los sistemas que recopilan, almacenan y procesan grandes cantidades de datos. Diseña y opera pipelines de datos, asegura la calidad de los flujos de información, y garantiza que los datos estén disponibles, confiables y listos para su análisis por otros equipos.\nImagina que eres responsable de construir y mantener las carreteras de una ciudad para que los camiones puedan transportar mercancías de forma eficiente y segura. El ingeniero de datos hace lo mismo pero con información.\nEn una empresa como Amazon o Mercado Libre, un ingeniero de datos construye los sistemas que recopilan millones de transacciones diarias, las procesan para eliminar errores, las transforman a formatos estándares, y las almacenan en diferentes bases de datos donde los analistas y científicos de datos pueden consultarlas. También asegura que estos procesos funcionen las 24 horas sin interrupciones y que los datos lleguen a tiempo para generar reportes y tomar decisiones críticas.\nData Analyst Es el profesional que explora, analiza y encuentra patrones en los datos para obtener conocimiento y responder preguntas de negocio. Utiliza estadísticas, herramientas de visualización y técnicas analíticas para comprender el pasado y el presente. Su objetivo es transformar datos en información útil que ayude a tomar decisiones informadas.\nImagina que eres un detective investigando un caso: examinas pistas, buscas patrones, y armas una historia lógica de lo que sucedió. Un analista de datos hace lo mismo con números y hechos.\nEn una tienda de ropa en línea, un analista de datos examinaría las ventas del último trimestre para identificar qué productos se venden más, en qué días de la semana hay más tráfico, qué regiones compran más, y cuál es el ticket promedio de compra. Con esta información, el equipo de marketing puede diseñar campañas más efectivas y el equipo de inventario puede planificar mejor sus compras.\nData Scientist Es similar al analista de datos, pero la diferencia es que los científicos de datos utilizan técnicas estadísticas sólidas y aprendizaje automático para predecir el futuro. (Los analistas son el pasado y el presente, los científicos son el futuro).\nSi quieres predecir el clima, primero tienes que entender por qué algunas regiones reciben más lluvia que otras, luego recopilar datos sobre temperatura, patrones, etc. Y luego usar herramientas para analizar y predecir cuándo lloverá, o usar herramientas de toma de decisiones para ver si es seguro viajar.\nData Architect Es el profesional responsable de diseñar, estructurar y organizar los activos de datos físicos y lógicos de una organización, así como los recursos de gestión de datos. Define cómo se almacenan, integran, acceden y consumen los datos a través de diferentes sistemas. Es el arquitecto del ecosistema de datos de una empresa.\nImagina que quieres construir una casa: necesitas un plano que muestre dónde irán las habitaciones, la cocina, las puertas, y cómo se conectan entre sí. Un arquitecto de datos hace lo mismo pero con información.\nEn una empresa de seguros, el arquitecto de datos diseñaría cómo los datos de pólizas, clientes, siniestros y pagos se almacenan en diferentes sistemas, cómo se conectan entre sí, qué estándares de seguridad aplicar, y cómo los diferentes equipos pueden acceder eficientemente a la información que necesitan sin duplicar datos ni comprometer la privacidad.\n","permalink":"http://localhost:1313/posts/data-terms/","summary":"Un listado de términos en el mundo de los datos que actualizo conforme voy aprendiendo nuevos conceptos.","title":"Data Terms"},{"content":"TL;DR Un Arquitecto de Datos diseña la estructura de almacenamiento, organización y utilización de datos para garantizar la calidad y flexibilidad de los mismos. Se enfoca en la estrategia y tácticas, trabajando en estrecha colaboración con ingenieros de datos para implementar la arquitectura. Comprender el ciclo de vida de la ingeniería de datos y los desafíos comunes es clave para agregar el valor empresarial necesario.\nIntro Hoy en día, es esencial que los datos puedan ser accedidos y reutilizados no solo por profesionales, sino también por usuarios comunes, como aquellos que utilizan redes sociales y tratan de descargar su propia información.\nTareas como crear, guardar, ingerir, transformar, procesar y visualizar datos requieren tiempo y esfuerzo.\nAdemás, mantener la seguridad, ejecutar la gestión, realizar operaciones, detallar la orquestación, preservar buenas prácticas de ingeniería de software y definir la estrategia de arquitectura de datos son aspectos críticos del ecosistema de datos.\nDe todos estos puntos, quiero centrarme en la arquitectura de datos. Existen varios roles relacionados con los datos en el mercado laboral que a veces pueden resultar abrumadores o difíciles de distinguir entre sí.\nSi tienes experiencia trabajando con datos, es posible que hayas notado que a veces un Analista de Datos realiza tareas que típicamente son realizadas por un Ingeniero de Datos, o viceversa.\nDe manera similar, un Científico de Datos puede estar enfocado únicamente en consultas SQL y paneles para una empresa. Esta variación en los roles a menudo depende de la madurez de los datos de la empresa, organización o proyecto.\n¿Qué es un Arquitecto de Datos? Si buscas en línea, en libros o preguntas a alguien con experiencia en datos, puedes encontrar diferentes definiciones de lo que es un arquitecto de datos.\nSegún el libro Deciphering Data Architectures de James Serra:\nSon quienes diseñan la estructura de alto nivel de la arquitectura de datos (MDW, Data Fabric o Data Lakehouse) y deciden qué tecnologías y políticas de gobernanza de datos debe utilizar el proyecto.\nSegún TOGAF:\nSon responsables de describir la estructura e interacción de los principales tipos y fuentes de datos, activos de datos lógicos, activos de datos físicos y recursos de gestión de datos de la empresa.\nSegún DAMA DMBOK:\nIdentifican las necesidades de datos de la empresa (independientemente de la estructura) y diseñan y mantienen planes maestros para guiar la integración de datos, controlar los activos de datos y alinear las inversiones en datos con la estrategia empresarial.\nSegún el libro Fundamentals of Data Engineering de Joe Ries \u0026amp; Matt Housley:\nFuncionan como un nivel de abstracción respecto a los ingenieros de datos. Los arquitectos de datos diseñan el modelo para la gestión organizacional de los datos, mapeando procesos y la arquitectura general de datos y sistemas. También sirven como un puente entre los aspectos técnicos y no técnicos de una organización.\nTambién tienen otra definición:\nSon aquellos que diseñan sistemas para apoyar las cambiantes necesidades de datos de una empresa, logradas a través de decisiones flexibles y reversibles alcanzadas mediante una cuidadosa evaluación de compensaciones. Ahora, ¿qué definición se debe elegir? Resumiendo todas estas definiciones, se podría encapsular de la siguiente manera:\nDiseñan la estructura general de cómo se almacenan, organizan y utilizan los datos. Deciden qué tecnologías se utilizarán para gestionar los datos. Crean reglas y políticas para garantizar que los datos sean de alta calidad y confiables. Aseguran que los sistemas de datos sean flexibles y puedan adaptarse a las cambiantes necesidades de la empresa. Funcionan como diseñadores, desarrollando los sistemas necesarios para todo el ciclo de vida de los datos, de modo que las empresas puedan maximizar y potenciar el valor de sus datos.\nUn arquitecto de datos analiza los pros y los contras, diseña con agilidad y agrega valor al negocio.\n¿Qué NO es un Arquitecto de Datos? Cualquier cosa relacionada tanto con la estrategia como con las tácticas puede considerarse parte de la Arquitectura de Datos.\nLa estrategia implica las preguntas qué, por qué y cuándo, mientras que las táctica involucra la pregunta cómo.\nSupongamos que alguien de tu empresa se acerca a ti y expresa la necesidad de integrar información de diversas fuentes para poder utilizar los datos.\nComo arquitecto de datos, debes comenzar investigando con qué exactamente están tratando, entendiendo por qué quieren esta integración y utilización de datos, y determinando cuándo necesitan este tipo de solución.\nEstas no son las únicas preguntas que un arquitecto de datos formula, pero el punto que quiero enfatizar es que no debes simplemente decir: \u0026ldquo;Tenemos las herramientas X, Y y Z para extraer y analizar datos,\u0026rdquo; ya que esto puede tener consecuencias negativas en el diseño de la solución.\nAdemás de la estrategia y las tácticas, los arquitectos de datos deben considerar tres aspectos principales al desarrollar una nueva arquitectura de datos:\nCompletitud Precisión Consistencia A menudo, los interesados pueden no tener una comprensión clara de lo que están tratando, por lo que es esencial estar presente al definir esos requisitos funcionales.\nEn mi opinión, es un arte; a veces, los interesados pueden saber solo que necesitan integrar y utilizar datos, lo cual puede ser un buen punto de partida.\n¿Pero estas tareas suelen ser realizadas por un Ingeniero de Datos, o no? Es cierto que un ingeniero de datos es capaz de manejar todas estas tareas, pero como se mencionó anteriormente, los arquitectos de datos operan a un nivel más alto de abstracción.\nAdemás, aunque no soy un experto en construcción, entiendo que un ingeniero civil podría cumplir con las funciones de un arquitecto. Entonces, ¿por qué existen los arquitectos?\nPorque los arquitectos se enfocan en la estrategia y las tácticas, mientras que los ingenieros dan vida a los diseños.\nUn ingeniero de datos tiene la tarea de crear, probar y mantener la arquitectura de datos. Escribe scripts para extraer, cargar y transformar datos de diversas fuentes para crear una solución de datos, trabajando en estrecha colaboración con un arquitecto de datos para implementar la arquitectura planificada.\nSi bien un ingeniero puede realizar trabajos de arquitectura, es importante definir sus limitaciones, objetivos, tareas y alcance. Esto es crucial porque es posible que hayas encontrado situaciones en tu lugar de trabajo donde un ingeniero asume más responsabilidades de las necesarias.\nAhora, de todos estos roles, ¿has observado alguna posición donde las tareas se hayan combinado?\nData Analyst Data Scientist Business Intelligence Analyst Data Engineer Database Administrator Data Architect Data Steward Product Owner No es raro que los roles se mezclen, pero es esencial recordar que un rol relacionado con los datos puede abarcar múltiples responsabilidades.\nSu Rol en el ciclo de vida de Ingenieria de Datos Entender el ciclo de vida de la ingeniería de datos es importante debido a su impacto en cada etapa de un proyecto, lo que en última instancia entrega valor empresarial a los interesados.\nGeneración Almacenamiento Ingestión Transformación Visualización o Entrega Los datos tienen valor en cada fase de este ciclo, y los datos que no son consumidos o consultados pueden representar un riesgo para cualquier negocio.\nMuchas empresas, en su búsqueda de proyectos ambiciosos en la era del big data, han recopilado cantidades masivas de datos que, en última instancia, no se utilizaron.\nLos proyectos deben ser intencionales a lo largo de todo el ciclo de vida, tanto en ingeniería como en datos.\nUn ingeniero de datos es responsable de extraer información de manera oportuna, siguiendo los protocolos adecuados de seguridad e integración, y cualquier otra tarea necesaria.\n¿Pero el trabajo de un Ingeniero de Datos termina aquí? Eso es lo que quiero que entiendas y a lo que me refiero: un arquitecto de datos debe evaluar, diseñar, organizar y ver el valor en las fases del proyecto. Un ingeniero de datos podría realizar todas estas tareas, pero no es su trabajo principal.\nPor ejemplo, imaginemos que vas a un hospital y te encuentras con un médico. Probablemente sepas con certeza que el médico va a:\nExaminar tu condición Hacer un diagnóstico Prescribir medicación Y seguramente no va a:\nLimpiar el hospital Fabricar la medicación Gestionar la administración del hospital ¿Pero qué pasa con los ingenieros de datos? ¿Las expectativas son poco claras? Lo que ocurre es que ellos pueden:\nDiseñar el modelo de datos de un Data Warehouse Gestionar bases de datos de aplicaciones Crear un pipeline de datos para Machine Learning Administrar toda la infraestructura de big data e instalación de software Analizar big data para transformar datos en bruto en información significativa Como mencioné anteriormente, si las empresas fueran más maduras en términos de datos, entonces habría límites mejor definidos.\nAl final del día, también depende de cómo las herramientas en lo que se conoce como \u0026ldquo;red de datos moderna\u0026rdquo; aumenten con la creciente complejidad de los datos.\n¿Qué es lo que debería de hacer un Ingeniero de Datos? Creo que deberían tener un amplio entendimiento de todo el ciclo de vida de los datos, distinguiendo entre habilidades esenciales, beneficiosas u opcionales.\nSin embargo, lo que se considera esencial en una empresa puede verse como opcional en otra. En última instancia, depende de las operaciones específicas y las necesidades del negocio y sus clientes.\nDesafíos Comunes de Datos Si la madurez de los datos es baja, puedes encontrar:\nSilos de datos Infraestructura inadecuada Resistencia cultural Vulnerabilidades de seguridad Por otro lado, si la madurez de los datos es alta, puedes enfrentar:\nProblemas de gobernanza y gestión de datos Innovación con tecnologías emergentes Desarrollo y mantenimiento de análisis avanzados En ambos escenarios, puede haber una variedad de desafíos, pero es importante reconocer que muchos datos no se están utilizando de manera efectiva. Esto podría deberse a un mal diseño de la arquitectura de datos, gobernanza o gestión de datos inadecuadas, o a una mala calidad de los datos desde su creación.\nOutro Un Arquitecto de Datos desempeña un papel crucial en la configuración del paisaje de datos de una organización. Son responsables de crear arquitecturas de datos robustas que aseguren la accesibilidad, confiabilidad y seguridad de los datos.\nAbordando tanto aspectos estratégicos como tácticos, los Arquitectos de Datos cierran la brecha entre los requisitos funcionales y los requisitos técnicos.\nSu capacidad para evaluar compensaciones y desarrollar sistemas adaptables es esencial para satisfacer las necesidades de datos.\nPor último, los Arquitectos de Datos empoderan a las organizaciones para utilizar eficazmente sus recursos de datos, impulsando el valor empresarial y la innovación.\n","permalink":"http://localhost:1313/posts/data-architect/","summary":"Enterate el por qué un Arquitecto de Datos puede tener un gran impacto y cuál es la diferencia entre otros roles en el mundo de los datos.","title":"¿Por qué ser un Arquitecto de Datos?"},{"content":"Contexto: La arquitectura zero-disk y el procesamiento single-node desafían una década de supuestos sobre “big data” al demostrar que la simplicidad supera a la complejidad distribuida para la mayoría de casos reales.\nPREGUNTA Tu equipo de data engineering ejecuta un dashboard diario que analiza 7 días de transacciones (100GB en Parquet) en un cluster Spark de 5 nodos que cuesta $3,000/mes.\nLas queries tardan 15-45 segundos, incluyendo 5-20 segundos de overhead de inicialización del cluster.\nUn cowoker propone migrar a DuckDB serverless en Lambda/Fargate con los datos en S3, estimando costos de $100-200/mes y tiempos de 5-15 segundos.\nEl CFO pregunta por qué esto sería más rápido si S3 tiene latencias de 10-100ms vs sub-milisegundo de discos locales. ¿Cuál es la razón técnica fundamental?\nA) S3 usa compresión automática que reduce el volumen de datos transferidos en 90%\nB) El overhead de coordinación distribuida (shuffles, serialización, startup) supera la penalización de latencia de S3 para queries \u0026lt;100GB\nC) Lambda tiene acceso directo a memoria RAM de S3 mediante VPC endpoints privados\nD) DuckDB usa GPU acceleration que compensa la latencia de red de object storage\nRESPUESTA: B\nEXPLICACIÓN La respuesta B es correcta porque para queries que escanean menos de 100GB, los sistemas distribuidos gastan 5-20 segundos en overhead de coordinación: inicialización del cluster, network shuffles entre nodos, serialización/deserialización de datos, y latencia del scheduler JVM.\nSingle-node elimina todo este overhead, dedicando 100% del tiempo a procesamiento real. Aunque S3 tiene mayor latencia I/O (10-100ms vs \u0026lt;1ms de discos locales), el caching agresivo (80-95% hit rates) y la ejecución vectorizada con formato columnar Parquet mitigan este impacto.\nDuckDB ejecuta queries en 5-15 segundos versus 15-45 de Spark porque evita toda la coordinación distribuida innecesaria para este volumen de datos.\nLas otras opciones son incorrectas:\nA) La compresión Parquet es independiente del storage backend\nB) Lambda no tiene acceso directo a “RAM de S3” (concepto inexistente)\nD) DuckDB usa ejecución vectorizada en CPU con instrucciones SIMD, no GPU acceleration.\nEJEMPLO REAL Un hedge fund reemplazó queries de 5 años de historial de trading (500GB Parquet en S3) ejecutadas en cluster Spark con tiempos de 5+ minutos (incluyendo espera de recursos) por DuckDB local en MacBook Pro completando las mismas agregaciones complejas en 30 segundos.\nEl analista eliminó la dependencia del equipo de ingeniería y el tiempo de espera por asignación de cluster.\nWarpStream aplicó este principio a streaming: su arquitectura zero-disk con agentes stateless y datos en S3 cuesta $1,000/mes versus $5,000/mes de Kafka tradicional (10 brokers + 20TB storage), aceptando 20-50ms de latencia versus 5ms de Kafka.\nCONSEJO CLAVE Audita tus patrones de queries antes de asumir que necesitas sistemas distribuidos. Si el 90% escanea menos de 100GB, single-node será más rápido y 80-95% más económico.\nImplementa particionamiento por tiempo (diario/mensual) para mantener los working sets pequeños y las queries naturalmente filtren a datos recientes.\nMide primero, distribuye después solo si es necesario.\nREFERENCIAS Big Data is Dead - MotherDuck - Análisis del CEO de MotherDuck (ex-director de BigQuery) sobre por qué 90% de workloads no necesitan infraestructura distribuida The Rise of Single-Node Processing: Challenging the Distributed Computing Paradigm - Casos de uso y benchmarks reales mostrando mejoras de 4X a 200X versus sistemas distribuidos​ WarpStream S3 Express One Zone Benchmark and Total Cost of Ownership - Comparación detallada de costos zero-disk: $2,961/mes WarpStream vs $8,223/mes Kafka optimizado ","permalink":"http://localhost:1313/posts/zero-disk-processing/","summary":"Cómo la simplicidad en almacenamiento y procesamiento redefine el costo y rendimiento en la analítica moderna.","title":"Pregunta #1 - Zero-Disk vs Distributed Processing"},{"content":"Contexto: ELT revolucionó la ingeniería de datos al mover transformaciones al warehouse, pero creó un problema crítico: sin entornos de desarrollo local ni staging, los data engineers testean todo directamente en producción.\nPREGUNTA Tu equipo maneja 800 transformaciones dbt en Snowflake. Cada vez que debuggean un error, el warehouse se bloquea 20 minutos y cuesta $500 por ejecución completa. Un bug reciente en el paso 92 de 103 requirió 8 iteraciones de debugging (total: $4,000 y 160 minutos). Tu VP de Ingeniería pregunta cómo reducir estos costos.\n¿Cuál es la solución más efectiva a largo plazo? A) Implementar más herramientas de observabilidad y monitoring para detectar errores más rápido antes de que lleguen a producción\nB) Contratar más data engineers para distribuir la carga de trabajo y reducir el tiempo de debugging por persona\nC) Establecer un entorno de staging con datos anonimizados y capacidades de desarrollo local antes de cualquier ejecución en warehouse\nD) Migrar a un warehouse más económico como BigQuery o Redshift para reducir el costo por query\nRESPUESTA: C\nEXPLICACIÓN La opción C ataca la causa raíz del problema: testing en producción sin staging. Un entorno de staging permite validar transformaciones, detectar errores de schema y probar lógica compleja en un ambiente aislado que replica la estructura de producción sin el costo ni riesgo de ejecutar en el warehouse real.\nCon herramientas como dlt+ Cache, los ingenieros pueden ejecutar transformaciones localmente con feedback instantáneo, validar antes de cargar datos, y reducir costos de debugging de horas a minutos.\nEsto implementa el mismo workflow que software engineering: test local → validación en staging → deploy a producción, eliminando la práctica peligrosa de testear directamente en producción.​\n¿Por qué las otras son incorrectas?\nLa opción A solo detecta problemas reactivamente pero no previene el testing costoso en producción. La opción B escala personas pero no resuelve el proceso ineficiente subyacente—más ingenieros solo distribuyen el mismo dolor costoso. La opción D reduce costos marginalmente pero mantiene el problema fundamental: cada iteración sigue siendo un test de producción completo.​\nEJEMPLO REAL Un líder de datos comentó: “Pasamos tres años intentando ahorrar dinero evitando staging environments. Terminamos gastando diez veces lo que ahorramos lidiando con issues de producción”.\nLa arquitectura de workspace de herramientas modernas proporciona separación limpia de entornos con pricing basado en uso real, no en row counts arbitrarios, haciendo que staging sea técnicamente más simple y financieramente viable.\nTeams que implementan staging antes de alcanzar cientos de pipelines previenen deuda técnica masiva que es exponencialmente más difícil de retrofitear en sistemas maduros.\nCONSEJO CLAVE Calcula el costo real del testing en producción: tiempo de warehouse + horas de ingeniería + costo de oportunidad + riesgo de corrupción de datos.\nEste cálculo construye un business case convincente para invertir en infraestructura de desarrollo apropiada antes de que escalar se vuelva operacionalmente imposible.\nREFERENCIAS Best practices for ETL and ELT pipelines - dbt Labs Building Data Trust Through Effective ETL Staging Environments - Matatika Staging Layer for Data Transformations - dlt Documentation ","permalink":"http://localhost:1313/posts/etl-escala-testing/","summary":"Por qué tu pipeline de dbt está costando $500 por iteración","title":"Pregunta #2 - ELT a Escala y Testing en Producción"},{"content":"Contexto: La elección del orquestador de workflows es una decisión arquitectónica de largo plazo que impacta la escalabilidad de tus pipelines, productividad del equipo y capacidad para adoptar prácticas modernas de gobernanza y observabilidad de datos.\nPREGUNTA Tu equipo en una fintech está migrando de scripts manuales a un orquestador enterprise. Los requisitos son: ejecutar 450 DAGs con tareas heterogéneas (Python, Bash, Spark, dbt), integración con Kubernetes para ML training, y el equipo de compliance exige trazabilidad completa de linaje de datos desde raw hasta reportes regulatorios sin herramientas externas. El CISO requiere SSO y RBAC en producción, pero el presupuesto solo cubre licencias open source este año.\n¿Qué arquitectura y herramienta deberías seleccionar para balancear estos requisitos contradictorios? A) Airflow – Maneja la heterogeneidad de tareas y tiene SSO/RBAC nativo en la versión open source, pero requerirás implementar linaje manualmente con herramientas como OpenLineage o Marquez\nB) Dagster – Proporciona linaje automático y assets como first-class citizens, perfecto para compliance, pero SSO y RBAC están bloqueados en la versión enterprise de pago\nC) Prefect – Ofrece orquestación reactiva en tiempo real y arquitectura moderna, pero carece de linaje nativo de datos y las features de seguridad requieren licencia enterprise\nD) Kestra – Combina flexibilidad de workflows heterogéneos con YAML declarativo, pero tiene comunidad pequeña (\u0026lt;10 contribuidores activos) y SSO/RBAC solo en versión enterprise\nRESPUESTA: A\nEXPLICACIÓN Airflow es la única opción que satisface el requisito no negociable de seguridad (SSO/RBAC en open source) porque es verdadero software Apache foundation-backed, no open core. Con 31M de descargas mensuales, 77K organizaciones usándolo, y ecosistema maduro para Kubernetes (KubernetesPodOperator usado por 60%+ de usuarios), maneja perfectamente la heterogeneidad de tareas requerida.\nEl trade-off del linaje manual es manejable: puedes integrar OpenLineage para tracking automático o implementar lineage a medida que compliance lo requiera, sin bloquear el despliegue inicial.\nPor qué las otras opciones fallan:\nDagster (B) es técnicamente superior para linaje automático y compliance, pero el modelo open core bloquea SSO/RBAC detrás de paywall enterprise, violando el constraint de presupuesto.\nPrefect (C) también usa open core y carece de linaje nativo, requiriendo implementación custom.\nKestra (D) tiene comunidad pequeña (\u0026lt;10 contribuidores) en zona de riesgo para viabilidad long-term y también bloquea features de seguridad.\nEJEMPLO REAL Uber ejecuta 450K tareas diarias con Airflow, Stripe orquesta 150K tasks daily, y LinkedIn maneja 10K DAGs paralelos simultáneos—todos casos que demuestran la capacidad enterprise-grade de Airflow para escalar con tareas heterogéneas.\nGoogle Cloud Composer y Amazon MWAA (con soporte para Airflow 3.1 desde noviembre 2025) seleccionaron Airflow como estándar managed, validando su madurez arquitectónica.\nPara el linaje de datos, organizaciones complementan Airflow con OpenLineage (open source Apache project) obteniendo trazabilidad completa sin licencias enterprise adicionales.\nCONSEJO CLAVE Antes de comprometerte con un orquestador, verifica la completitud de features en open source versus enterprise. Los modelos open core (Dagster, Prefect, Kestra, Mage) bloquean seguridad crítica (SSO, RBAC, audit logs) detrás de paywalls, creando costos ocultos post-deployment.\nEvalúa salud de comunidad con métricas concretas: 20+ contribuidores activos es saludable, \u0026lt;5 es zona de riesgo. Airflow tiene 3K+ contribuidores versus \u0026lt;10 en alternativas emergentes, garantizando sostenibilidad long-term del investment.\nREFERENCIAS State of Apache Airflow 2025 - 31M monthly downloads y 77K organizaciones\nComparativa arquitecturas task-centric vs data-centric en orquestación 2025\nAirflow 3.1 Human-in-the-Loop workflows para AI - release septiembre 2025\n","permalink":"http://localhost:1313/posts/arq-orquestacion-task-data/","summary":"¿Cuándo usar Airflow, Dagster o Prefect según tus necesidades?","title":"Pregunta #3 - Arquitecturas de Orquestación: Task-Centric vs Data-Centric"},{"content":"Contexto: En proyectos de búsqueda semántica y RAG, muchos equipos asumen que necesitan bases de datos vectoriales especializadas desde el inicio, sin considerar alternativas más simples para conjuntos de datos pequeños y medianos.\nPREGUNTA Estás desarrollando un sistema de búsqueda semántica para documentación interna de tu empresa con aproximadamente 25,000 artículos. Ya generaste embeddings de 768 dimensiones usando un modelo BERT y necesitas decidir cómo almacenarlos. El equipo de infraestructura está preocupado por los costos operativos, pero el equipo de producto requiere latencias sub-5ms para búsquedas con filtros por categoría y fecha. Tu arquitectura debe ser portable entre ambientes de desarrollo local y producción en AWS.\n¿Cuál es el enfoque más apropiado para este escenario?\nA) Implementar Qdrant con índices HNSW en Docker, priorizando latencia sub-1ms sobre simplicidad operativa\nB) Usar archivos Parquet con Polars para búsqueda lineal, aprovechando el formato columnar y conversión zero-copy a NumPy\nC) Almacenar embeddings en PostgreSQL con extensión pgvector y índices HNSW para obtener 471 QPS a escala\nD) Guardar embeddings en archivos CSV con pandas por familiaridad del equipo y facilidad de debugging\nRESPUESTA: B\nEXPLICACIÓN La opción B es correcta porque 25,000 vectores con 768 dimensiones ocupan aproximadamente 73MB en memoria (25k × 768 × 4 bytes), un tamaño manejable donde la búsqueda lineal alcanza latencias de 1-2ms según benchmarks reales.\nParquet ofrece compresión 6x superior a CSV, soporte nativo para arrays tipados como List(Float32), y acceso columnar selectivo que permite cargar solo embeddings y metadatos necesarios.\nPolars proporciona conversión zero-copy a NumPy para operaciones vectorizadas de producto punto, eliminando overhead de serialización manual. Este enfoque cumple el requisito de sub-5ms, evita complejidad de Docker/configuración, y mantiene portabilidad perfecta mediante un único archivo.\nPor qué las otras opciones fallan:​\nLas opciones A y C son sobre-ingeniería prematura: bases de datos vectoriales con HNSW se justifican después de ~100k vectores donde búsqueda lineal O(n) se vuelve prohibitiva.\nLa opción D es ineficiente: CSV consume 6x más espacio, requiere parsing completo del archivo, carece de tipos para arrays anidados, y pandas trata listas como objetos genéricos con penalizaciones severas de rendimiento versus el soporte nativo de Polars para pl.List.\nEJEMPLO REAL El proyecto Magic: The Gathering implementó búsqueda sobre 32,254 cartas con embeddings de 768D usando esta arquitectura exacta, logrando queries filtradas por metadatos en 1.48ms.\nEl sistema permite búsquedas semánticas complejas como “cartas similares a Lightning Helix pero solo Sorceries con mana Black” aplicando filtros de Polars antes de calcular similitud coseno mediante producto punto en NumPy.\nTodo funciona desde un archivo Parquet portable sin infraestructura de base de datos, demostrando que 94MB de embeddings (32k × 768 × 4 bytes) operan perfectamente en memoria de laptops modernas con latencias aceptables para usuarios finales.\nCONSEJO CLAVE Benchmarkea tu caso de uso real antes de adoptar bases de datos vectoriales. La regla práctica es: si tu dataset completo cabe en memoria (~100k vectores o menos), búsqueda lineal con Parquet+Polars ofrece latencias sub-2ms sin overhead operacional.\nPre-normaliza embeddings a longitud unitaria durante almacenamiento para convertir similitud coseno en simple producto punto, acelerando queries significativamente.\nSolo escala a soluciones indexadas (HNSW/IVF) cuando datasets excedan 100k vectores o necesites consultas distribuidas.\nREFERENCIAS Almacenamiento portátil de embeddings con Parquet y Polars - minimaxir.com Formato columnar Parquet: beneficios y mejores prácticas - Airbyte Interoperabilidad zero-copy entre Polars y NumPy - Polars Blog ","permalink":"http://localhost:1313/posts/almacenamiento-embeddings-parquet/","summary":"¿Cuándo necesitas realmente una base de datos vectorial?","title":"Pregunta #4 - Almacenamiento Portátil de Embeddings con Parquet"},{"content":"El término Big Data surgió hace años y desde entonces muchos consideran que los datos son el \u0026ldquo;nuevo petróleo\u0026rdquo;. Pero en la realidad, aprovechar plenamente este recurso es complicado: abundan los retos en gestión, calidad, integridad y privacidad.\nLos datos terminan siendo preocupación secundaria en vez de un activo crítico.\n\u0026ldquo;What You See Is All There Is\u0026rdquo; – Daniel Kahneman, Thinking Fast and Slow\nEsta frase resume un peligro común: tomar decisiones importantes solo con la información que tenemos a la vista, sin examinar qué falta o podría estar mal.\nPor eso, los datos no son museos de dashboards ni monstruos que nos generen desconfianza, sino una fuente valiosa cuando se tratan con rigor y conciencia.\n¿Qué encontrarás aquí? Preguntas técnicas sobre ingeniería de datos, ML/AI y gestión moderna de datos para practicar y aprender en minutos. Artículos prácticos y análisis sobre calidad, arquitectura, seguridad y mejores prácticas del manejo de datos en la vida real. Reflexiones, ejemplos y consejos que abordan problemas cotidianos del mundo de datos: gestión, visualización, acceso, privacidad y toma de decisiones. ¿Quién soy y por qué hago esto? Soy Omar Valdez, he trabajado desde la trinchera armando reportes y pipelines hasta definir arquitecturas y liderar proyectos de datos a escala.\nHe visto de cerca la importancia (y los desafíos) de tratar a los datos como capital estratégico, no solo como \u0026ldquo;lo que soporta la operación\u0026rdquo;. Estoy convencido de que una comunidad informada y consciente puede potenciar el valor de los datos, reducir riesgos y tomar decisiones más inteligentes.\nMi compromiso contigo Promover mejores prácticas para extraer valor real, cuidando siempre la integridad y confidencialidad de los datos Compartir contenido útil, claro y sin teoría innecesaria, siempre enfocado en la aplicación práctica Hacer visible el lado humano, ético y estratégico de los datos ","permalink":"http://localhost:1313/whoami/","summary":"\u003cp\u003eEl término Big Data surgió hace años y desde entonces muchos consideran que los datos son el \u0026ldquo;nuevo petróleo\u0026rdquo;. Pero en la realidad, aprovechar plenamente este recurso es complicado: abundan los retos en gestión, calidad, integridad y privacidad.\u003c/p\u003e\n\u003cp\u003eLos datos terminan siendo preocupación secundaria en vez de un activo crítico.\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u0026ldquo;What You See Is All There Is\u0026rdquo;\n– Daniel Kahneman, Thinking Fast and Slow\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003eEsta frase resume un peligro común: tomar decisiones importantes solo con la información que tenemos a la vista, sin examinar qué falta o podría estar mal.\u003c/p\u003e","title":"\u003e_ whoami"},{"content":"La razón principal por la que estoy haciendo esto es porque esperaba poder saber, durante el proceso de personalizar mi interfaz de usuario, cómo hacerlo lo más atractivo y eficiente posible.\nComo lo comentó Linus Torvalds:\n\u0026ldquo;Cualquier programa solo es tan bueno como sea útil. En el verdadero open source, tienes el derecho a controlar tu propio destino.\u0026rdquo;\nNOTA: Generalmente se actualiza de forma eventual, por lo que NO puede estar al día esta información.\nLo Básico Para mi sistema operativo, uso MacOs Para mi window manager, uso Aerospace. Para mi terminal, uso Ghostty. Para mi shell, uso Zsh. Aplicaciones Para mi launcher, uso Raycast Para mi editor, uso NeoVim Para mi file manager, uso Yazi Para mi terminal multiplexer, uso Tmux Theme Para mi theme, generalmente uso Tokyonight Storm Para mi terminal font, uso Iosevka Nerd Font Para mi shell prompt, uso oh-my-posh Si deseas ver más de mis dotfiles, haz click aquí\n","permalink":"http://localhost:1313/dotfiles/","summary":"\u003cp\u003eLa razón principal por la que estoy haciendo esto es porque esperaba poder saber, durante el proceso de personalizar mi interfaz de usuario, cómo hacerlo lo más atractivo y eficiente posible.\u003c/p\u003e\n\u003cp\u003eComo lo comentó Linus Torvalds:\u003c/p\u003e\n\u003cblockquote\u003e\n\u003cp\u003e\u003cem\u003e\u0026ldquo;Cualquier programa solo es tan bueno como sea útil. En el verdadero open source, tienes el derecho a controlar tu propio destino.\u0026rdquo;\u003c/em\u003e\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cp\u003e\u003cem\u003eNOTA: Generalmente se actualiza de forma eventual, por lo que NO puede estar al día esta información.\u003c/em\u003e\u003c/p\u003e","title":"dotfiles"}]
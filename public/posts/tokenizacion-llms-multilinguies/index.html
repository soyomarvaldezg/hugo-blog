<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes | Omar Valdez G</title><meta name=keywords content="tokenization,llms,tokens"><meta name=description content="Optimizaci√≥n de Costos y Contexto"><meta name=author content="Omar Valdez G"><link rel=canonical href=http://localhost:1313/posts/tokenizacion-llms-multilinguies/><link crossorigin=anonymous href=/assets/css/stylesheet.098c413488605d1532e70aee11c86f3752a3892cc096ba30d2d3d8a85384b054.css integrity="sha256-CYxBNIhgXRUy5wruEchvN1KjiSzAlrow0tPYqFOEsFQ=" rel="preload stylesheet" as=style><script defer crossorigin=anonymous src=/assets/js/highlight.f413e19d0714851f6474e7ee9632408e58ac146fbdbe62747134bea2fa3415e0.js integrity="sha256-9BPhnQcUhR9kdOfuljJAjlisFG+9vmJ0cTS+ovo0FeA=" onload=hljs.initHighlightingOnLoad()></script><link rel=icon href=http://localhost:1313/favicon/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/favicon/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/favicon/favicon-16x16.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/posts/tokenizacion-llms-multilinguies/><noscript><style>#theme-toggle,.top-link{display:none}</style></noscript><link rel=preconnect href=https://fonts.googleapis.com><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link href="https://fonts.googleapis.com/css2?family=Fira+Code&display=swap" rel=stylesheet><script src=https://unpkg.com/typeit@8.7.1/dist/index.umd.js></script><style>body{font-family:iosevka nerd font bold,monospace}</style><meta property="og:title" content="Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes"><meta property="og:description" content="Optimizaci√≥n de Costos y Contexto"><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/posts/tokenizacion-llms-multilinguies/"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-11-21T00:00:00+00:00"><meta property="article:modified_time" content="2025-11-21T00:00:00+00:00"><meta property="og:site_name" content="Omar Valdez G"><meta name=twitter:card content="summary"><meta name=twitter:title content="Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes"><meta name=twitter:description content="Optimizaci√≥n de Costos y Contexto"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:1313/posts/"},{"@type":"ListItem","position":2,"name":"Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes","item":"http://localhost:1313/posts/tokenizacion-llms-multilinguies/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes","name":"Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes","description":"Optimizaci√≥n de Costos y Contexto","keywords":["tokenization","llms","tokens"],"articleBody":"Contexto: La tokenizaci√≥n es el primer paso cr√≠tico donde los LLMs convierten texto en unidades num√©ricas (tokens). Este proceso determina directamente los costos de la API y cu√°nto texto cabe en la ventana de contexto, afectando dr√°sticamente el rendimiento en aplicaciones multiling√ºes.\nPREGUNTA Est√°s construyendo un sistema de RAG que procesa documentos en ingl√©s y japon√©s usando un modelo basado en SentencePiece. Notas que, para la misma cantidad de informaci√≥n, los documentos en japon√©s consumen casi el doble de tokens, elevando los costos de la API y reduciendo el contexto √∫til.\n¬øCu√°l es la raz√≥n fundamental de esta ineficiencia?\nA) La arquitectura del Transformer no est√° optimizada para caracteres no latinos.\nB) Los embeddings vectoriales para el japon√©s requieren m√°s dimensiones, consumiendo m√°s memoria.\nC) La tokenizaci√≥n de lenguas sin espacios claros (como el japon√©s) es inherentemente menos eficiente, generando m√°s tokens por car√°cter.\nD) Los modelos de SentencePiece aplican un recargo multiplicativo a los tokens no ingleses.\nRESPUESTA: C\nEXPLICACI√ìN La respuesta correcta es C porque la ineficiencia radica en el propio proceso de tokenizaci√≥n, no en el modelo. SentencePiece, al no depender de pre-tokenizaci√≥n por espacios, trata el texto a nivel de caracteres o sub-palabras. Lenguajes como el japon√©s, que no usan espacios como delimitadores y tienen miles de caracteres Kanji, se fragmentan en muchos m√°s tokens en comparaci√≥n con el ingl√©s, donde una palabra promedio es 0.75 tokens. Esta mayor cantidad de tokens es la causa directa del aumento de costos (pagas por token) y la reducci√≥n del contexto disponible (caben menos tokens en la ventana).\nLas otras opciones son incorrectas porque la arquitectura Transformer y las dimensiones del embedding son generalmente constantes independientemente del idioma. Aunque el precio de la API puede variar, la causa t√©cnica fundamental es la mayor generaci√≥n de tokens, no una pol√≠tica de precios. La ineficiencia ocurre antes de que los datos lleguen al modelo.\nEJEMPLO REAL OpenAI reporta que el texto en ingl√©s usa aproximadamente 1 token por cada 4 caracteres, mientras que el mismo texto en chino o japon√©s puede necesitar 2-3 veces m√°s tokens. Un documento de 1000 palabras en ingl√©s puede ocupar ~1330 tokens, pero su traducci√≥n al japon√©s f√°cilmente supera los 2500 tokens, reduciendo a menos de la mitad la capacidad de un contexto de 8k tokens y duplicando el costo de procesamiento en la API de un proveedor como Azure OpenAI.\nCONSEJO CLAVE Antes de desplegar un sistema multiling√ºe, utiliza herramientas como el ‚Äútokenizer playground‚Äù de OpenAI o Hugging Face para analizar la tokenizaci√≥n real de tus textos en cada idioma. Presupuesta los tokens usando un factor de 2-3x para lenguajes no latinos y considera tokenizadores especializados si el costo es cr√≠tico\nREFERENCIAS https://github.com/openai/tiktoken\nhttps://huggingface.co/learn/llm-course/en/chapter2/4\nhttps://airbyte.com/data-engineering-resources/llm-tokenization\n","wordCount":"450","inLanguage":"en","datePublished":"2025-11-21T00:00:00Z","dateModified":"2025-11-21T00:00:00Z","author":{"@type":"Person","name":"Omar Valdez G"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/posts/tokenizacion-llms-multilinguies/"},"publisher":{"@type":"Organization","name":"Omar Valdez G","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon/favicon.ico"}}}</script></head><body class=dark id=top><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="~$ cd .. (Alt + H)"><img src=http://localhost:1313/favicon/favicon.ico alt aria-label=logo height=35>~$ cd ..</a><div class=logo-switches><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/search/ title=üîé><span>üîé</span></a></li><li><a href=http://localhost:1313/dotfiles/ title="‚Ä¢ dots"><span>‚Ä¢ dots</span></a></li><li><a href=http://localhost:1313/tags/ title="‚Ä¢ tags"><span>‚Ä¢ tags</span></a></li><li><a href=http://localhost:1313/newsletter title="‚Ä¢ newsletter"><span>‚Ä¢ newsletter</span></a></li><li><a href=http://localhost:1313/posts/ title="‚Ä¢ posts"><span>‚Ä¢ posts</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><h1 class=post-title>Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes</h1><div class=post-meta></div></header><div class=toc><details open><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><ul><li><a href=#pregunta aria-label=PREGUNTA>PREGUNTA</a></li><li><a href=#explicaci%c3%b3n aria-label=EXPLICACI√ìN>EXPLICACI√ìN</a></li><li><a href=#ejemplo-real aria-label="EJEMPLO REAL">EJEMPLO REAL</a></li><li><a href=#consejo-clave aria-label="CONSEJO CLAVE">CONSEJO CLAVE</a></li><li><a href=#referencias aria-label=REFERENCIAS>REFERENCIAS</a></li></ul></div></details></div><div class=post-content><p><em><strong>Contexto</strong>: La tokenizaci√≥n es el primer paso cr√≠tico donde los LLMs convierten texto en unidades num√©ricas (tokens). Este proceso determina directamente los costos de la API y cu√°nto texto cabe en la ventana de contexto, afectando dr√°sticamente el rendimiento en aplicaciones multiling√ºes.</em></p><h1 id=pregunta>PREGUNTA<a hidden class=anchor aria-hidden=true href=#pregunta>#</a></h1><p>Est√°s construyendo un sistema de RAG que procesa documentos en ingl√©s y japon√©s usando un modelo basado en SentencePiece. Notas que, para la misma cantidad de informaci√≥n, los documentos en japon√©s consumen casi el doble de tokens, elevando los costos de la API y reduciendo el contexto √∫til.</p><p><strong>¬øCu√°l es la raz√≥n fundamental de esta ineficiencia?</strong></p><p>¬†</p><p>A) La arquitectura del Transformer no est√° optimizada para caracteres no latinos.</p><p>B) Los embeddings vectoriales para el japon√©s requieren m√°s dimensiones, consumiendo m√°s memoria.</p><p>C) La tokenizaci√≥n de lenguas sin espacios claros (como el japon√©s) es inherentemente menos eficiente, generando m√°s tokens por car√°cter.</p><p>D) Los modelos de SentencePiece aplican un recargo multiplicativo a los tokens no ingleses.</p><p>¬†</p><p><strong>RESPUESTA: C</strong></p><h1 id=explicaci√≥n>EXPLICACI√ìN<a hidden class=anchor aria-hidden=true href=#explicaci√≥n>#</a></h1><p>La respuesta correcta es C porque la ineficiencia radica en el propio proceso de tokenizaci√≥n, no en el modelo. SentencePiece, al no depender de pre-tokenizaci√≥n por espacios, trata el texto a nivel de caracteres o sub-palabras. Lenguajes como el japon√©s, que no usan espacios como delimitadores y tienen miles de caracteres Kanji, se fragmentan en muchos m√°s tokens en comparaci√≥n con el ingl√©s, donde una palabra promedio es 0.75 tokens. Esta mayor cantidad de tokens es la causa directa del aumento de costos (pagas por token) y la reducci√≥n del contexto disponible (caben menos tokens en la ventana).</p><p>Las otras opciones son incorrectas porque la arquitectura Transformer y las dimensiones del embedding son generalmente constantes independientemente del idioma. Aunque el precio de la API puede variar, la causa t√©cnica fundamental es la mayor generaci√≥n de tokens, no una pol√≠tica de precios. La ineficiencia ocurre antes de que los datos lleguen al modelo.</p><h1 id=ejemplo-real>EJEMPLO REAL<a hidden class=anchor aria-hidden=true href=#ejemplo-real>#</a></h1><p>OpenAI reporta que el texto en ingl√©s usa aproximadamente 1 token por cada 4 caracteres, mientras que el mismo texto en chino o japon√©s puede necesitar 2-3 veces m√°s tokens. Un documento de 1000 palabras en ingl√©s puede ocupar ~1330 tokens, pero su traducci√≥n al japon√©s f√°cilmente supera los 2500 tokens, reduciendo a menos de la mitad la capacidad de un contexto de 8k tokens y duplicando el costo de procesamiento en la API de un proveedor como Azure OpenAI.</p><h1 id=consejo-clave>CONSEJO CLAVE<a hidden class=anchor aria-hidden=true href=#consejo-clave>#</a></h1><p>Antes de desplegar un sistema multiling√ºe, utiliza herramientas como el ‚Äútokenizer playground‚Äù de OpenAI o Hugging Face para analizar la tokenizaci√≥n real de tus textos en cada idioma. Presupuesta los tokens usando un factor de 2-3x para lenguajes no latinos y considera tokenizadores especializados si el costo es cr√≠tico</p><h1 id=referencias>REFERENCIAS<a hidden class=anchor aria-hidden=true href=#referencias>#</a></h1><ul><li><p><a href=https://github.com/openai/tiktoken>https://github.com/openai/tiktoken</a></p></li><li><p><a href=https://huggingface.co/learn/llm-course/en/chapter2/4>https://huggingface.co/learn/llm-course/en/chapter2/4</a></p></li><li><p><a href=https://airbyte.com/data-engineering-resources/llm-tokenization>https://airbyte.com/data-engineering-resources/llm-tokenization</a></p></li></ul></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/tokenization/>Tokenization</a></li><li><a href=http://localhost:1313/tags/llms/>Llms</a></li><li><a href=http://localhost:1313/tags/tokens/>Tokens</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/posts/modern-etl-testing-dbt-sqlmesh/><span class=title>¬´ Prev</span><br><span>Pregunta #5 - Modern ELT Testing en Producci√≥n: dbt vs. SQLMesh</span>
</a><a class=next href=http://localhost:1313/posts/arquitectura-medallion-warehouse/><span class=title>Next ¬ª</span><br><span>Pregunta #7 - Arquitectura Medallion en Data Lakehouse</span></a></nav><div id=cusdis_thread data-host=https://cusdis.com data-app-id=f9b750aa-55fe-4da9-82cf-45aa511cea20 data-app-id=414adb98-9af8-4403-bdc5-ba55ae808bea data-page-id=491537a3b6fbb13b596c13c86463374e data-page-url=http://localhost:1313/posts/tokenizacion-llms-multilinguies/ data-page-title="Pregunta #6 - Tokenizaci√≥n en LLMs Multiling√ºes"></div><script async defer src=https://cusdis.com/js/cusdis.es.js></script></footer></article></main><script async defer src=https://scripts.withcabin.com/hello.js></script></body></html>